# -*- coding: utf-8 -*-
"""MULTICLASSSPROJECT_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-KJxf8CmVxI79LnYLgV3fQe5Oi2Qpwg

# **Prediction target:**
Multi-class classification across 7 different emotions from thouands of audio files from Ravdess,TESS and CREMA-D datasets <br>
{neutral, happy, sad, angry, fear, disgust, surprise}
"""

# === A. MOUNT DRIVE (once per session) ===
from google.colab import drive, auth
drive.mount('/content/drive', force_remount=False)

DRIVE_ROOT        = "/content/drive/MyDrive/multiclassproject"
RAVDESS_DIR_DRIVE = f"{DRIVE_ROOT}/Audio_Speech_Actors_01-24"
TESS_DIR_DRIVE    = f"{DRIVE_ROOT}/TESS Toronto emotional speech set data"

# === A. MOUNT DRIVE & DEFINE PATHS ===
from google.colab import drive
import os

# Mount Google Drive (if not already mounted)
if not os.path.ismount('/content/drive'):
    drive.mount('/content/drive', force_remount=False)

# --- Define paths for datasets in Google Drive ---
DRIVE_ROOT        = "/content/drive/MyDrive/multiclassproject"
RAVDESS_DIR_DRIVE = f"{DRIVE_ROOT}/Audio_Speech_Actors_01-24"
TESS_DIR_DRIVE    = f"{DRIVE_ROOT}/TESS Toronto emotional speech set data"

# --- Path to the compressed CREMA-D file in Drive ---
CREMA_TAR_DRIVE   = f"{DRIVE_ROOT}/CREMA-D.tar.gz"

# --- LOCAL path where CREMA-D will be extracted for FAST access ---
# The tar file contains a directory named 'AudioWAV'.
CREMA_DIR_LOCAL   = "/content/AudioWAV"

print("✅ Paths defined.")
print(f"RAVDESS (Drive): {RAVDESS_DIR_DRIVE}")
print(f"TESS (Drive):    {TESS_DIR_DRIVE}")
print(f"CREMA-D (TAR):   {CREMA_TAR_DRIVE}")


# === B. EXTRACT CREMA-D TO LOCAL DISK ===
# Check if the tar file exists before trying to extract
if os.path.exists(CREMA_TAR_DRIVE):
    print(f"\nFound '{os.path.basename(CREMA_TAR_DRIVE)}'.")
    print("Extracting to local Colab disk for faster processing...")

    # Use !tar to unzip the file directly into the /content/ directory
    !tar -xzf "{CREMA_TAR_DRIVE}" -C /content/

    print(f"✅ Extraction complete. CREMA-D data is ready at: {CREMA_DIR_LOCAL}")
else:
    print(f"❌ ERROR: The file was not found at {CREMA_TAR_DRIVE}")

"""# **Project Setup & Naming Conventions**"""

# === TO SPEED-UP PROCESSING (All datasets via rsync) ===
import os
import subprocess
import shlex

PROJECT_NAME = "speech_emotion_ravdess_tess_crema"
DATASET_NAME = "ravdess_tess_crema_7cls"

# ---------- Ensure Drive + define source folders ----------
def ensure_drive_paths():
    """Defines and validates the source directories in Google Drive."""
    global DRIVE_ROOT, RAVDESS_DIR_DRIVE, TESS_DIR_DRIVE, CREMA_DIR_DRIVE
    if not os.path.ismount("/content/drive"):
        from google.colab import drive
        drive.mount("/content/drive", force_remount=False)

    DRIVE_ROOT = "/content/drive/MyDrive/multiclassproject"
    RAVDESS_DIR_DRIVE = f"{DRIVE_ROOT}/Audio_Speech_Actors_01-24"
    TESS_DIR_DRIVE = f"{DRIVE_ROOT}/TESS Toronto emotional speech set data"
    # Path to the CREMA-D FOLDER in Drive, as seen in your screenshot
    CREMA_DIR_DRIVE = f"{DRIVE_ROOT}/crema"

    assert os.path.isdir(RAVDESS_DIR_DRIVE), f"Missing: {RAVDESS_DIR_DRIVE}"
    assert os.path.isdir(TESS_DIR_DRIVE), f"Missing: {TESS_DIR_DRIVE}"
    assert os.path.isdir(CREMA_DIR_DRIVE), f"Missing: {CREMA_DIR_DRIVE}"
    print("✅ All source directories found in Google Drive.")

ensure_drive_paths()

# ---------- Local workspace (FAST) ----------
LOCAL_ROOT = f"/content/data/{PROJECT_NAME}"
LOCAL_RAVDESS = f"{LOCAL_ROOT}/RAVDESS"
LOCAL_TESS = f"{LOCAL_ROOT}/TESS"
LOCAL_CREMA = f"{LOCAL_ROOT}/CREMA"  # Consistent local directory name
os.makedirs(LOCAL_RAVDESS, exist_ok=True)
os.makedirs(LOCAL_TESS, exist_ok=True)
os.makedirs(LOCAL_CREMA, exist_ok=True)

# --- knobs ---
FORCE_RESYNC = False

# ---------- helpers ----------
def _count_wavs(path: str) -> int:
    """Counts .wav files in a given directory path."""
    if not os.path.isdir(path):
        return 0
    try:
        out = subprocess.check_output(f'find "{path}" -type f -iname "*.wav" | wc -l', shell=True).decode().strip()
        return int(out or 0)
    except subprocess.CalledProcessError:
        return 0

def _mirror_rsync(src: str, dst: str, label: str):
    """Mirrors a source directory to a destination using rsync."""
    os.makedirs(dst, exist_ok=True)
    args = ['rsync', '-a', '--delete', '--info=progress2', src.rstrip('/') + '/', dst]
    print(f"[{label}] Mirroring Drive → Local (rsync)...")
    subprocess.run(args, check=True)

# ---------- main: ensure all local data is synced via rsync ----------
def ensure_local_data():
    """Checks if local data matches Drive and syncs if necessary."""
    print("--- Checking status of local dataset copies ---")
    # Get file counts from both Drive and local for all three datasets
    rav_drive_count = _count_wavs(RAVDESS_DIR_DRIVE)
    tess_drive_count = _count_wavs(TESS_DIR_DRIVE)
    crema_drive_count = _count_wavs(CREMA_DIR_DRIVE)

    rav_local_count = _count_wavs(LOCAL_RAVDESS)
    tess_local_count = _count_wavs(LOCAL_TESS)
    crema_local_count = _count_wavs(LOCAL_CREMA)

    # Check if any of the datasets are incomplete locally
    needs_sync = (
        FORCE_RESYNC or
        rav_local_count < rav_drive_count * 0.98 or
        tess_local_count < tess_drive_count * 0.98 or
        crema_local_count < crema_drive_count * 0.98
    )

    if needs_sync:
        print("One or more datasets are incomplete locally. Syncing all datasets...")
        _mirror_rsync(RAVDESS_DIR_DRIVE, LOCAL_RAVDESS, "RAVDESS")
        _mirror_rsync(TESS_DIR_DRIVE,    LOCAL_TESS,    "TESS")
        _mirror_rsync(CREMA_DIR_DRIVE,   LOCAL_CREMA,   "CREMA-D")
    else:
        print("✅ All dataset copies are up-to-date. Skipping sync.")

ensure_local_data()

# ---------- Use LOCAL paths from now on ----------
RAVDESS_DIR = LOCAL_RAVDESS
TESS_DIR = LOCAL_TESS
CREMA_DIR = LOCAL_CREMA

# ---------- Inventory (after sync) ----------
rav_count = _count_wavs(RAVDESS_DIR)
tess_count = _count_wavs(TESS_DIR)
crema_count = _count_wavs(CREMA_DIR)
print("\nREADY — working locally on the VM SSD")
print(f"RAVDESS local: {RAVDESS_DIR}  -> {rav_count} wavs")
print(f"TESS    local: {TESS_DIR}     -> {tess_count} wavs")
print(f"CREMA-D local: {CREMA_DIR}    -> {crema_count} wavs")

# === 1. IMPORTS & CONFIG ===
import os, glob, re, random, warnings

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

import librosa
import librosa.display
from IPython.display import Audio
from tqdm import tqdm

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    balanced_accuracy_score,
    roc_auc_score,
    cohen_kappa_score,
    matthews_corrcoef,
    classification_report,
    confusion_matrix
)

"""**RAW FILE STRUCTURE & LABELS**<br>
**Objective:**


*  To show how RAVDESS,TESS and CREMA-D emotion filenames were encoded.
*  Detect the raw labels each dataset exposes
*  To understand how the mapping sequence should be done to be able to merge the two set of files.<br> (Naming and set of emotion categories)

"""

# === 1A. RAW INSPECTION (Corrected for nested CREMA-D) ===

CREMA_AUDIO_DIR = os.path.join(CREMA_DIR, "AudioWAV")
print(f"Corrected path for CREMA-D audio: {CREMA_AUDIO_DIR}")


def sample_files(root, n=8):
    out = []
    for d,_,files in os.walk(root):
        for fn in files:
            if fn.lower().endswith(".wav"):
                out.append(os.path.join(d, fn))
                if len(out) >= n:
                    return out
    return out

print("\nSample RAVDESS files:")
for p in sample_files(RAVDESS_DIR): print("  ", os.path.basename(p))

print("\nSample TESS files:")
for p in sample_files(TESS_DIR): print("  ", os.path.basename(p))

# Use the corrected path to sample files
print("\nSample CREMA-D files:")
for p in sample_files(CREMA_AUDIO_DIR): print("  ", os.path.basename(p))


# --- Extract RAW labels straight from filenames (dataset-native) ---
RAVDESS_EMOTION_CODEBOOK = {
    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
    '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprise'
}

def ravdess_raw_labels(ravdess_dir):
    rows = []
    for d,_,files in os.walk(ravdess_dir):
        for fn in files:
            if not fn.lower().endswith(".wav"): continue
            parts = fn.split('-')
            label = RAVDESS_EMOTION_CODEBOOK.get(parts[2]) if len(parts) > 2 else None
            rows.append({"dataset":"RAVDESS","path":os.path.join(d,fn),"raw_label":label,"file":fn})
    return pd.DataFrame(rows)

def tess_raw_labels(tess_dir):
    rows = []
    for d,_,files in os.walk(tess_dir):
        for fn in files:
            if not fn.lower().endswith(".wav"): continue
            emotion = fn.split('_')[-1].split('.')[0] if '_' in fn else None
            rows.append({"dataset":"TESS","path":os.path.join(d,fn),"raw_label":emotion,"file":fn})
    return pd.DataFrame(rows)

# MODIFIED to handle the nested 'AudioWAV' folder
def crema_raw_labels(crema_audio_dir):
    rows = []
    # Use os.walk to robustly find files in subdirectories
    for d, _, files in os.walk(crema_audio_dir):
        for fn in files:
            if not fn.lower().endswith(".wav"): continue
            # Filename: 1001_DFA_ANG_XX.wav -> Emotion is the 3rd part
            parts = fn.split('_')
            label = parts[2] if len(parts) > 2 else None
            rows.append({"dataset":"CREMA-D", "path":os.path.join(d, fn), "raw_label":label, "file":fn})
    return pd.DataFrame(rows)


rav_raw = ravdess_raw_labels(RAVDESS_DIR)
tes_raw = tess_raw_labels(TESS_DIR)
# Call the function with the corrected path
crema_raw = crema_raw_labels(CREMA_AUDIO_DIR)

raw_df = pd.concat([rav_raw, tes_raw, crema_raw], ignore_index=True)

print("\n--- Distinct RAW labels per dataset (BEFORE any cleaning/mapping) ---")
for name, sub in raw_df.groupby("dataset"):
    print(f"\n{name}:")
    print(sub["raw_label"].value_counts(dropna=False).sort_index())

print("\n--- Quick mismatch check (RAW) ---")
rav_set_raw = set(rav_raw["raw_label"].dropna().unique())
tes_set_raw = set(tes_raw["raw_label"].dropna().unique())
crema_set_raw = set(crema_raw["raw_label"].dropna().unique())

print("RAVDESS raw set:", sorted(rav_set_raw))
print("TESS    raw set:", sorted(tes_set_raw))
print("CREMA-D raw set:", sorted(crema_set_raw))
print("\n--- Differences ---")
print("Only-in-RAVDESS (vs all):", sorted(rav_set_raw - tes_set_raw - crema_set_raw))
print("Only-in-TESS    (vs all):", sorted(tes_set_raw - rav_set_raw - crema_set_raw))
print("Only-in-CREMA-D (vs all):", sorted(crema_set_raw - rav_set_raw - tes_set_raw))

"""**Initial cleaning of the dataset format**

1.   Lowercasing all labels

  *   Example:Ensures "Happy" and "happy" are treated the same
  *   This removes differences caused by inconsistent capital letters

2.   Removing all leading/trailing spaces

  *   Removes extra spaces "happy_" with a space to avoid being counted as a separate label from "happy"

3.   Normalization of common variations

  *   Example:converting "pleasant_surprise" to "ps" so we can see it as the same shorthand form used in other files.
  *   This is still not mapping to "surprise" yet — just making the form consistent.

**Outcome:**
  *   This step usually reduces the number of unique labels (because duplicates collapse).
  *   It makes the set difference check between datasets more accurate — now it highlights real mismatches instead of superficial ones.

"""

# === 1B. LIGHT CLEANING —  ===
# Objective: show how simple normalization changes the comparison

def normalize_for_display(s):
    return (
        s.astype(str)          # handle None/NaN uniformly
         .str.strip()
         .str.lower()
         .str.replace('pleasant_surprise', 'ps', regex=False)  # optional: unify common variations
    )

clean_df = raw_df.copy()
clean_df["raw_label_clean"] = normalize_for_display(clean_df["raw_label"])

print("\n--- Distinct labels after LIGHT CLEAN (lowercase/strip only) ---")
# This loop already works for all datasets, including CREMA-D
for name, sub in clean_df.groupby("dataset"):
    print(f"\n{name} (clean view):")
    print(sub["raw_label_clean"].value_counts(dropna=False).sort_index())

# --- MODIFIED FOR 3-WAY COMPARISON ---
print("\n--- Quick mismatch check (CLEAN VIEW) ---")
rav_set_clean = set(clean_df.loc[clean_df.dataset=="RAVDESS","raw_label_clean"].dropna().unique())
tes_set_clean = set(clean_df.loc[clean_df.dataset=="TESS","raw_label_clean"].dropna().unique())
# ADDED FOR CREMA-D
crema_set_clean = set(clean_df.loc[clean_df.dataset=="CREMA-D","raw_label_clean"].dropna().unique())

print("RAVDESS clean set:", sorted(rav_set_clean))
print("TESS    clean set:", sorted(tes_set_clean))
print("CREMA-D clean set:", sorted(crema_set_clean)) # ADDED FOR CREMA-D
print("\n--- Differences (CLEAN VIEW) ---")
print("Only-in-RAVDESS (vs all):", sorted(rav_set_clean - tes_set_clean - crema_set_clean))
print("Only-in-TESS    (vs all):", sorted(tes_set_clean - rav_set_clean - crema_set_clean))
print("Only-in-CREMA-D (vs all):", sorted(crema_set_clean - rav_set_clean - tes_set_clean))

"""**Mapping of filenames on the dataset to native labels.** <br>

This step converts the raw labels from each dataset's filenames into a standardized set of emotion words. The process differs per dataset.

*   For RAVDESS: Decodes numeric emotion codes (e.g., '03') into full words ('happy') using its specific codebook.
  > RAVDESS_EMOTION_CODEBOOK = {
    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
    '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprise'
}
*   For TESS dataset, extracts the word (e.g., happy, ps) directly from the end of the filename.

*   For CREMA-D: Similarly, decodes three-letter abbreviations (e.g., 'HAP') into full words ('happy') using the CREMA-D codebook.

*   For all datasets: The final label_mapped is created by converting the resulting word to lowercase and stripping any extra spaces to ensure consistency across the entire collection.

"""

# === DATA LOADING — DATASET-NATIVE MAPPING ===


# Ensure we are pointing to the correct subdirectory containing the audio files.
CREMA_AUDIO_DIR = os.path.join(CREMA_DIR, "AudioWAV")

RAVDESS_EMOTION_CODEBOOK = {
    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
    '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprise'
}

# ADDED FOR CREMA-D: Maps 3-letter code to the emotion word.
CREMA_EMOTION_CODEBOOK = {
    'ANG': 'angry',
    'DIS': 'disgust',
    'FEA': 'fear',
    'HAP': 'happy',
    'NEU': 'neutral',
    'SAD': 'sad'
}

def load_ravdess_native(ravdess_dir):
    paths, raw = [], []
    for dirname, _, files in os.walk(ravdess_dir):
        for fn in files:
            if not fn.lower().endswith(".wav"):
                continue
            parts = fn.split('-')
            label = RAVDESS_EMOTION_CODEBOOK.get(parts[2]) if len(parts) > 2 else None
            paths.append(os.path.join(dirname, fn))
            raw.append(label)
    df = pd.DataFrame({"dataset":"RAVDESS","path":paths,"label_raw":raw})
    df["label_mapped"] = df["label_raw"].astype(str).str.strip().str.lower()
    return df

def load_tess_native(tess_dir):
    paths, raw = [], []
    for dirname, _, files in os.walk(tess_dir):
        for fn in files:
            if not fn.lower().endswith(".wav"):
                continue
            token = fn.split('_')[-1].split('.')[0] if '_' in fn else None
            paths.append(os.path.join(dirname, fn))
            raw.append(token)
    df = pd.DataFrame({"dataset":"TESS","path":paths,"label_raw":raw})
    df["label_mapped"] = df["label_raw"].astype(str).str.strip().str.lower()
    return df

# ADDED FOR CREMA-D
def load_crema_native(crema_audio_dir):
    paths, raw = [], []
    for dirname, _, files in os.walk(crema_audio_dir):
        for fn in files:
            if not fn.lower().endswith(".wav"):
                continue
            # Filename: 1001_DFA_ANG_XX.wav -> Emotion code is the 3rd part
            parts = fn.split('_')
            code = parts[2] if len(parts) > 2 else None
            label = CREMA_EMOTION_CODEBOOK.get(code) # Use the codebook to get the full word
            paths.append(os.path.join(dirname, fn))
            raw.append(label) # label_raw will be 'angry', 'sad', etc.
    df = pd.DataFrame({"dataset":"CREMA-D","path":paths,"label_raw":raw})
    df["label_mapped"] = df["label_raw"].astype(str).str.strip().str.lower()
    return df

rav_native = load_ravdess_native(RAVDESS_DIR)
tess_native = load_tess_native(TESS_DIR)
crema_native = load_crema_native(CREMA_AUDIO_DIR) # ADDED

# MODIFIED to include CREMA-D
native_df = pd.concat([rav_native, tess_native, crema_native], ignore_index=True)

print("--- Dataset-native labels (label_mapped) ---")
for name, sub in native_df.groupby("dataset"):
    print(f"\n{name} counts:")
    print(sub["label_mapped"].value_counts(dropna=False).sort_index())

"""**Merging of all datasets into a  7-class emotion category** <br>
Objective: convert both datasets’ labels to the same final vocabulary for training.

*   calm → neutral
*   fearful → fear
*   ps or pleasant_surprise → surprise
*   also ensure all lowercased

"""

# === HARMONIZATION — map dataset-native labels to final taxonomy ===

FINAL_TAXONOMY = ['neutral','happy','sad','angry','fear','disgust','surprise']

HARMONIZE_MAP = {
    # normalize synonyms/variants
    'fearful': 'fear',
    'pleasant_surprise': 'surprise',
    'ps': 'surprise',
    # merge categories as decided
    'calm': 'neutral',
}

def harmonize_labels(df):
    out = df.copy()
    # start from label_mapped (already lowercase/strip)
    out["label_final"] = out["label_mapped"].replace(HARMONIZE_MAP)
    # keep everything lowercase just in case
    out["label_final"] = out["label_final"].astype(str).str.strip().str.lower()
    return out

harm_df = harmonize_labels(native_df)

print("\n--- 2B: Harmonized labels (label_final) ---")
for name, sub in harm_df.groupby("dataset"):
    print(f"\n{name} counts:")
    print(sub["label_final"].value_counts(dropna=False).sort_index())

"""# Merging of labels and emotions for both datasets"""

# === VALIDATION & FILTERING TO FINAL TAXONOMY ===

def filter_to_final_taxonomy(df, allowed=FINAL_TAXONOMY):
    before = len(df)
    keep_mask = df["label_final"].isin(allowed)
    dropped = (~keep_mask).sum()
    filtered = df[keep_mask].reset_index(drop=True)
    print(f"\nKept {len(filtered)}/{before} files. Dropped {dropped} not in final taxonomy.")
    print("Final label set:", sorted(filtered["label_final"].unique()))
    return filtered

df = filter_to_final_taxonomy(harm_df)

# (Optional) tidy columns for modeling
df = df[["path","label_final"]].rename(columns={"label_final":"label"})

print("\nTotal files for training:", len(df))
print("Labels:", sorted(df['label'].unique()))
df.head()

"""# Second set of Data format cleaning

*   Remove missing paths or labels (dropna)
*   Remove duplicate file entries
  * This step prevents errors during model training and guarantees that each row corresponds to a valid, unique audio file with a clean label
*   Keep only files that still exist on disk (os.path.exists)
  * We checked that each path actually points to an existing .wav file.




"""

# === DATASET CLEANING ===
# Drop rows with missing path/label
df = df.dropna(subset=['path','label']).copy()

# Keep only files that actually exist
df = df[df['path'].apply(os.path.exists)].reset_index(drop=True)

# Drop duplicate file paths (safety)
df = df.drop_duplicates(subset=['path']).reset_index(drop=True)

print("After basic cleaning:", df.shape)

"""# **EDA of Audio files**
Loading of raw audio & diagnostics.

*   How files are stored (sample rate, channels)
*   How loud they are (peak amplitude)
*   How much silence they contain before and after speech

**Step-by-step flow of execution**
1. Loading the file as-is

  *   Librosa is used to load (librosa.load) and not to resample (sr=None) and not to force mono (mono=False), so we can get the original sample rate and channel count.
  *   This preserves the raw condition of the file for inspection.

2. Convert to mono for measurement for diagnostic purposes only

  *   Even though the file may be stereo, most of our stats (RMS, peak, silence) are easier to compare if they’re based on a single combined channel.

3. Calculate audio metrics

  *   Sample rate (sr): How many samples per second the audio was originally recorded at. This can vary a lot between datasets.
  *   Duration (seconds): How long the clip is from start to finish.

  *   Peak amplitude: The loudest point in the clip (absolute value). This shows whether the file is too quiet or too loud.
  *   RMS (Root Mean Square) level in dB: The average power/loudness of the clip. RMS is more reliable than peak for measuring perceived volume.

  *   Leading silence (ms): How much quiet time exists before speech starts.
  *   Trailing silence (ms): How much quiet time exists after speech ends.

    * We used librosa.effects.trim to detect where the non-silent portion begins and ends.
4. Return all metrics in a dictionary
  *  This makes it easy to print, store, or compare against later cleaning stages.

**Key points to look for:**

*   If the sample rates vary (e.g., 44.1 kHz, 48 kHz, 22.05 kHz) then resampling is required
*   If many audio files have 2-3 seconds of leading silence, then trimming of the silence is required.
  *   Trimming will save model training from learning "silence patterns"
*   If RMS levels are not consistent then normalization will help features focus on actual speech content instead of volume differences.
"""

def print_step_header(header_text):
    print(header_text)

# === 1. RAW LOAD (MULTI-SAMPLE VIEW, NO CLEANING) ===
print_step_header("STEP 1 — RAW LOAD (EDA - MULTI-SAMPLE VIEW)")

# Modified show_raw_samples to remove extra newlines before header
def show_raw_samples(samples, seconds_preview=2.0, header=""):
    if header:
        print("="*80)
        print(f"{header}")
        print("="*80)
    for emo, path in samples:
        print(f"\n### Emotion: {emo}")
        y, sr = load_raw(path)
        summarize_raw(y, sr, path)
        preview_waveform(y, sr, seconds=seconds_preview, title_extra=f" — {emo}")

random.seed(7)

# ---------- utilities you already have ----------
def load_raw(path):
    y, sr = librosa.load(path, sr=None, mono=False)
    return y, sr

def _hms(seconds: float) -> str:
    m, s = divmod(int(round(seconds)), 60)
    return f"{m:02d}:{s:02d}"

def summarize_raw(y, sr, path):
    if y.ndim == 2:
        channels = y.shape[0]; samples_per_ch = y.shape[1]
        dur = samples_per_ch / sr if sr else 0.0
        flat = y.reshape(-1)
        pcm  = [float(np.max(np.abs(y[c]))) for c in range(channels)]
        pmin = [float(np.min(y[c])) for c in range(channels)]
        pmean= [float(np.mean(y[c])) for c in range(channels)]
    else:
        channels = 1; samples_per_ch = y.shape[0]
        dur = samples_per_ch / sr if sr else 0.0
        flat = y
        pcm  = [float(np.max(np.abs(y)))]; pmin = [float(np.min(y))]; pmean = [float(np.mean(y))]

    try: fsize = os.path.getsize(path)
    except OSError: fsize = None

    clip_pct = 100.0 * float(np.mean(np.abs(flat) >= 0.999)) if flat.size else 0.0
    def _fmt(vals): return ", ".join([f"{v:.4f}" for v in vals])

    print("─"*72)
    print("RAW AUDIO SUMMARY")
    print("─"*72)
    print(f"File:                {path}")
    if fsize is not None: print(f"File size:           {fsize/1024:.1f} KB")
    print(f"Sample rate (Hz):    {sr}")
    print(f"Channels:            {channels}  ({'stereo' if channels==2 else 'mono'})")
    print(f"Samples / channel:   {samples_per_ch:,}")
    print(f"Duration:            {dur:.3f} sec  ({_hms(dur)})")
    print(f"Max amplitude:       {_fmt([abs(v) for v in pmin])} (min)  |  {_fmt(pcm)} (|max|)")
    print(f"Mean amplitude:      {_fmt(pmean)}  (DC offset check)")
    print(f"Clipping (>=0.999):  {clip_pct:.3f}% of samples")
    print("─"*72)

def preview_waveform(y, sr, seconds=2.0, title_extra=""):
    y_plot = librosa.to_mono(y) if y.ndim == 2 else y
    n = min(len(y_plot), int(seconds * sr))
    if n <= 0:
        print("Nothing to plot."); return
    plt.figure(figsize=(10, 3))
    plt.plot(np.arange(n) / sr, y_plot[:n])
    plt.xlabel("Time (s)"); plt.ylabel("Amplitude")
    plt.title(f"Raw waveform preview (first {min(seconds, n/sr):.2f}s){title_extra}")
    plt.tight_layout(); plt.show()

# --- PATH & CODEBOOK SETUP ---
# Re-define path to CREMA-D audio files to ensure this cell is self-contained
CREMA_AUDIO_DIR = os.path.join(CREMA_DIR, "AudioWAV")

TESS_EMOTIONS = [
    "angry", "disgust", "fear", "happy", "neutral", "pleasant_surprise", "sad"
]

RAVDESS_EMO_MAP = {
    "01": "neutral", "02": "calm", "03": "happy", "04": "sad",
    "05": "angry", "06": "fear", "07": "disgust", "08": "surprise"
}

# Add CREMA-D codebook, used by the new function below
CREMA_EMO_MAP = {
    'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fear',
    'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'
}

# --- FILE LISTING FUNCTIONS ---
def list_tess_by_emotion():
    # ... (this function is unchanged)
    emo_dict = {e: [] for e in TESS_EMOTIONS}
    for emo in TESS_EMOTIONS:
        for actor_folder in ["OAF", "YAF"]:
             patt = os.path.join(TESS_DIR, f"{actor_folder}_{emo}", "*.wav")
             emo_dict[emo].extend(glob.glob(patt))
    return {k: v for k, v in emo_dict.items() if v}

def list_ravdess_by_emotion():
    # ... (this function is unchanged)
    emo_dict = {e: [] for e in set(RAVDESS_EMO_MAP.values())}
    wavs = glob.glob(os.path.join(RAVDESS_DIR, "Actor_*", "*.wav"))
    for p in wavs:
        base = os.path.basename(p)
        parts = base.split('-')
        if len(parts) >= 3:
            code = parts[2]
            emo = RAVDESS_EMO_MAP.get(code)
            if emo: emo_dict[emo].append(p)
    return {k: v for k, v in emo_dict.items() if v}

# ADDED FOR CREMA-D
def list_crema_by_emotion():
    """Groups CREMA-D file paths by emotion using its codebook."""
    emo_dict = {e: [] for e in set(CREMA_EMO_MAP.values())}
    wavs = glob.glob(os.path.join(CREMA_AUDIO_DIR, "*.wav"))
    for p in wavs:
        base = os.path.basename(p)
        # Filename: 1001_DFA_ANG_XX.wav -> Emotion code is the 3rd part
        parts = base.split('_')
        if len(parts) >= 3:
            code = parts[2]
            emo = CREMA_EMO_MAP.get(code)
            if emo:
                emo_dict[emo].append(p)
    return {k: v for k, v in emo_dict.items() if v}

# --- SAMPLING & EXECUTION ---
def sample_per_emotion(file_dict, per_emotion=1):
    # ... (this function is unchanged)
    picks = []
    for emo, files in sorted(file_dict.items()):
        if not files: continue
        take = min(per_emotion, len(files))
        picks.extend([(emo, f) for f in random.sample(files, take)])
    return picks

# ===== Run: one sample per emotion from each dataset =====
tess_files = list_tess_by_emotion()
rav_files  = list_ravdess_by_emotion()
crema_files = list_crema_by_emotion() # ADDED

tess_samples = sample_per_emotion(tess_files, per_emotion=1)
rav_samples  = sample_per_emotion(rav_files,  per_emotion=1)
crema_samples = sample_per_emotion(crema_files, per_emotion=1) # ADDED

show_raw_samples(tess_samples, seconds_preview=2.0, header="TESS DATASET — one raw sample per emotion")
show_raw_samples(rav_samples,  seconds_preview=2.0, header="RAVDESS — one raw sample per emotion")
show_raw_samples(crema_samples, seconds_preview=2.0, header="CREMA-D — one raw sample per emotion") # ADDED

"""•  If mean is far from 0, then will need DC offset removal<p>
•  If max amplitude << 1.0, normalization will help<p>
•  Clipping > 0% suggests the waveform may be saturated.

**What’s DC offset?**

> Audio waveforms ideally oscillate evenly around 0 amplitude (center line in the waveform plot).
> Sometimes, due to recording hardware, electrical interference, or processing errors, the entire signal is shifted up or down.
> This shift is called a DC (Direct Current) offset, because it’s like a constant “voltage bias” added to the whole waveform.

Example:
```
Correct signal: -1.0 ... 0.0 ... +1.0
With DC offset: -0.8 ... +0.2 ... +1.2   (centered around +0.2 instead of 0.0)
```

**Why is it a problem?**<p>
**Wasted headroom:** The offset pushes part of the signal closer to clipping.

**Distortion risk:** Boosting/normalizing a biased signal may clip the positive or negative peaks.

**Inaccurate analysis:** Many audio features (RMS, spectral analysis) assume zero-centered data.

**Audio File Diagnostics**
The purpose of this script is to analyze the technical properties of a single audio file. It identifies potential quality issues and generates a report with specific recommendations for preprocessing steps. The process is handled by three main functions:


*   dataset_profile(path): This function first identifies the source dataset (TESS, RAVDESS, or CREMA-D) by inspecting the input file path. It then returns a profile containing standard parameters for that dataset, such as the target sample rate (e.g., 22050 Hz) and an expected duration range.
*   audio_stats(...): This function takes the raw audio data and calculates a dictionary of key technical metrics. These metrics include the sample rate, duration, peak amplitude, RMS (Root Mean Square) level for loudness, mean amplitude (to check for DC offset), and the amount of leading/trailing silence.
*   print_stats(...): It compares the actual metrics calculated by audio_stats against the standard parameters defined in dataset_profile. Based on this comparison, it prints a summary with explicit recommendations for cleaning actions, such as whether to resample, normalize, or trim the audio file.
"""

# ---- plain text step header (no borders, no colors)
def print_step_header(step_number, description):
    print(f"STEP {step_number} — {description}")

# === STEP 2 (with CREMA-D profile) ===
print_step_header(2, "DIAGNOSTICS ON SAMPLE DATA")

# ---- dataset-aware profile (targets & typical duration ranges) ----
def dataset_profile(path):
    p = path.lower()
    # Defaults if unknown
    profile = {
        "name": "dataset",
        "target_sr": 22050,
        "dur_range": (1.0, 4.0)  # seconds
    }
    if "tess" in p:
        profile.update({"name": "TESS", "target_sr": 22050, "dur_range": (1.2, 2.5)})
    elif "ravdess" in p or "actor_" in p:
        profile.update({"name": "RAVDESS", "target_sr": 22050, "dur_range": (2.5, 4.0)})
    # ADDED FOR CREMA-D
    elif "crema" in p:
        profile.update({"name": "CREMA-D", "target_sr": 22050, "dur_range": (1.5, 3.5)})
    return profile

def audio_stats(y, sr, trim_db=20):
    """Diagnostics only — no cleaning applied."""
    y_mono = librosa.to_mono(y) if y.ndim == 2 else y
    dur = len(y_mono) / sr if sr else 0.0
    peak = float(np.max(np.abs(y_mono))) if y_mono.size else 0.0
    mean_amp = float(np.mean(y_mono)) if y_mono.size else 0.0
    rms = float(np.sqrt(np.mean(y_mono**2))) if y_mono.size else 0.0
    rms_db = 20 * np.log10(rms + 1e-12)
    _, idx = librosa.effects.trim(y_mono, top_db=trim_db)
    lead_ms = (idx[0] / sr) * 1000
    trail_ms = ((len(y_mono) - idx[1]) / sr) * 1000
    clip_pct = 100.0 * float(np.mean(np.abs(y_mono) >= 0.999)) if y_mono.size else 0.0
    return {
        "Sample rate (Hz)": sr, "Duration (sec)": round(dur, 3),
        "Peak amplitude": round(peak, 4), "RMS (dB)": round(rms_db, 2),
        "Mean amplitude": round(mean_amp, 4), "Leading silence (ms)": int(lead_ms),
        "Trailing silence (ms)": int(trail_ms), "Clipping (%)": round(clip_pct, 3),
        "_y": y_mono, "_sr": sr
    }

def inspect_raw(path, trim_db=20):
    """Load file as-is and return diagnostics."""
    y, sr = librosa.load(path, sr=None, mono=False)
    return audio_stats(y, sr, trim_db=trim_db)

# --- helper: conditional figures ---
def _plot_sr_if_needed(sr, target_sr, title_suffix=""):
    if sr == target_sr: return
    plt.figure(figsize=(4, 2.5)); plt.bar(["current", "target"], [sr, target_sr])
    plt.title(f"Sample rate check {title_suffix}"); plt.ylabel("Hz")
    plt.tight_layout(); plt.show()

def _plot_duration_if_outside(dur, lo, hi, title_suffix=""):
    if lo <= dur <= hi: return
    plt.figure(figsize=(5.5, 2.5)); plt.bar(["duration"], [dur])
    plt.hlines([lo, hi], xmin=-0.5, xmax=0.5, linestyles=["dashed","dashed"])
    plt.text(0.55, lo, f"lo={lo:.2f}s", va="center"); plt.text(0.55, hi, f"hi={hi:.2f}s", va="center")
    plt.title(f"Duration vs typical range {title_suffix}"); plt.ylabel("seconds")
    plt.tight_layout(); plt.show()

def print_stats(stats_dict, title="Diagnostics", path_for_context=None):
    prof = dataset_profile(path_for_context or "")
    sr, dur, peak, rmsdb, mean, lead, trail, clip = (
        stats_dict["Sample rate (Hz)"], stats_dict["Duration (sec)"],
        stats_dict["Peak amplitude"], stats_dict["RMS (dB)"],
        stats_dict["Mean amplitude"], stats_dict["Leading silence (ms)"],
        stats_dict["Trailing silence (ms)"], stats_dict["Clipping (%)"]
    )
    print("\n" + title)
    need = f"→ resample to {prof['target_sr']} Hz for {prof['name']} consistency." if sr != prof['target_sr'] else "→ OK (matches target)."
    print(f"{'Sample rate (Hz)':<25}: {sr} {need}")
    _plot_sr_if_needed(sr, prof['target_sr'], title_suffix=f"({prof['name']})")
    lo, hi = prof["dur_range"]
    if dur < lo: dmsg = f"→ shorter than typical {prof['name']} range {lo:.2f}–{hi:.2f}s."
    elif dur > hi: dmsg = f"→ longer than typical {prof['name']} range {lo:.2f}–{hi:.2f}s."
    else: dmsg = f"→ OK (within typical {prof['name']} range {lo:.2f}–{hi:.2f}s)."
    print(f"{'Duration (sec)':<25}: {dur:.3f} {dmsg}")
    _plot_duration_if_outside(dur, lo, hi, title_suffix=f"({prof['name']})")
    if peak >= 0.99 or clip > 0.1: pmsg = "→ TOO HOT / likely clipping; normalize down or limit."
    elif peak <= 0.65: pmsg = "→ quiet; normalize up to target peak ≈ 0.95."
    else: pmsg = "→ reasonable; normalization optional."
    print(f"{'Peak amplitude':<25}: {peak:.4f} {pmsg}")
    if rmsdb < -30: rmsg = "→ very quiet; gain-up likely needed."
    elif rmsdb > -12: rmsg = "→ loud; verify no distortion."
    else: rmsg = "→ typical dialog range."
    print(f"{'RMS (dB)':<25}: {rmsdb:.2f} {rmsg}")
    if abs(mean) > 0.02: mmsg = "→ DC offset suspected; apply high-pass/DC removal."
    else: mmsg = "→ centered near zero."
    print(f"{'Mean amplitude':<25}: {mean:.4f} {mmsg}")
    s_thresh = 50  # ms
    lmsg = "→ trim recommended." if lead > s_thresh else "→ minimal."
    tmsg = "→ trim recommended." if trail > s_thresh else "→ minimal."
    print(f"{'Leading silence (ms)':<25}: {lead} {lmsg}")
    print(f"{'Trailing silence (ms)':<25}: {trail} {tmsg}")
    cmsg = "→ high; review peaks or re-record." if clip > 0.1 else "→ low."
    print(f"{'Clipping (%)':<25}: {clip:.3f} {cmsg}")

# ----- DEMO on one fixed file -----
# This path is unchanged, as you requested.
example_path = "/content/drive/MyDrive/multiclassproject/TESS Toronto emotional speech set data/OAF_angry/OAF_voice_angry.wav"

# Safety check
import os
assert os.path.exists(example_path), f"File not found: {example_path}"

# Run diagnostics
raw_stats = inspect_raw(example_path, trim_db=20)
print(f"File: {example_path}")
print_stats(raw_stats, title="BEFORE CLEANING (RAW)", path_for_context=example_path)

"""<H1>EXPLANATION OF RESULTS</H1>

**Duration (sec): 1.589** – The clip length is within the typical TESS dataset range of 1.20–2.50 seconds, so no trimming or padding is necessary.
---
**Peak amplitude: 0.1522** – The loudest point in the audio reaches about 15.2% of the maximum possible digital amplitude, which is quiet. It can be normalized up to a target peak of around 0.95 to increase volume without distortion.
---
<b>RMS (dB): -31.20 dB </b>– This is the average loudness level of the clip. It is quieter than the typical dialog range (around -30 to -12 dB), so a gain boost will help for better audibility.
---
<b>Mean amplitude: -0.0000 </b>– The average amplitude across the clip is  zero, indicating no DC offset and a waveform centered properly.
---
**Leading silence (ms): 62 ms** – A short pause exists at the start of the clip. Trimming can help align data for more consistent processing and training.
---
**Trailing silence (ms): 246 ms** – There’s a longer pause at the end of the clip. Trimming is recommended to remove unnecessary silence that does not contribute to the signal.
---
<b>Clipping (%): 0.000</b> – No samples exceed the maximum amplitude, so there’s no distortion from clipping.
--

# **Which files needed cleaning for the entire dataset?**

<h2>THRESHOLDS</h2>
<p><b>TARGET_SR = 22050</b></p>
A common speech/audio standard in librosa tutorials. It halves 44.1k/48k audio while keeping energy up to ~11 kHz (speech is mostly <8 kHz). It makes TESS (≈24.4k) and RAVDESS (48k) consistent and cheaper to process.

<p><b>TRIM_DB = 20</p></b>
Used by librosa.effects.trim as “keep anything within 20 dB of the peak.” It’s a conservative silence trim that won’t chop quiet consonants/breaths. (30 dB is more aggressive; 10 dB is gentler.)

<p><b>PEAK_MIN_FOR_OK = 0.90</p></b>
If the file’s peak amplitude is <0.9 (on the -1..1 scale), we normalize. It’s a simple trigger to get peaks close to full scale without overreacting to tiny headroom differences.

<p><b>ABS_MEAN_MAX_OK = 0.01</p></b>
DC-offset check. |mean| > 0.01 (≈ -40 dB) is a noticeable bias; we recentre. Smaller offsets are usually negligible.

<p><b>SILENCE_MS_OK = 100</p></b>
If leading or trailing silence >100 ms, we trim. ~0.1 s is audible dead air but short enough to keep natural pauses.

<p><b>CLIP_PCT_OK = 0.1</p></b>
If ≥0.1% of samples hit |x|≥0.999, we flag clipping (saturation risk). Anything non-zero is suspicious; 0.1% avoids false positives from single spikes.

<p><b> DUR_RANGE_OK = (1.0, 4.0)</p></b>
Typical utterance length in TESS/RAVDESS sits ~1–3 s. Using 1–4 s catches outliers (too short/long) without being harsh.
"""

# --- config (thresholds) ---
TARGET_SR = 22050
TRIM_DB   = 20
PEAK_MIN_FOR_OK = 0.90
ABS_MEAN_MAX_OK = 0.01
SILENCE_MS_OK   = 100
CLIP_PCT_OK     = 0.1
DUR_RANGE_OK    = (1.0, 4.0)


# ---- Minimal audio processing functions ----
def _audio_stats_min(y, sr, trim_db=TRIM_DB):
    y_mono = librosa.to_mono(y) if y.ndim == 2 else y
    dur = len(y_mono)/sr if sr else 0.0
    peak = float(np.max(np.abs(y_mono))) if y_mono.size else 0.0
    mean_amp = float(np.mean(y_mono)) if y_mono.size else 0.0
    rms = float(np.sqrt(np.mean(y_mono**2))) if y_mono.size else 0.0
    rms_db = 20*np.log10(rms + 1e-12)
    _, idx = librosa.effects.trim(y_mono, top_db=trim_db)
    lead_ms  = (idx[0] / sr) * 1000
    trail_ms = ((len(y_mono) - idx[1]) / sr) * 1000
    clip_pct = 100.0 * float(np.mean(np.abs(y_mono) >= 0.999)) if y_mono.size else 0.0
    return {
        "sr": sr, "duration_sec": round(dur,3), "peak_amp": round(peak,4),
        "rms_db": round(rms_db,2), "mean_amp": round(mean_amp,4),
        "leading_silence_ms": int(lead_ms), "trailing_silence_ms": int(trail_ms),
        "clipping_pct": round(clip_pct,3)
    }

def audio_stats(y, sr, trim_db=TRIM_DB):
    return _audio_stats_min(y, sr, trim_db)

def inspect_raw(path, trim_db=TRIM_DB):
    y, sr = librosa.load(path, sr=None, mono=False)
    return audio_stats(y, sr, trim_db=trim_db)

# ---- Build manifest if missing ----

# --- Define paths and codebooks needed to build the manifest ---
CREMA_AUDIO_DIR = os.path.join(CREMA_DIR, "AudioWAV")
TESS_EMOTIONS = ["angry","disgust","fear","happy","neutral","pleasant_surprise","sad"]
RAVDESS_EMO_MAP = {"01":"neutral","02":"calm","03":"happy","04":"sad","05":"angry","06":"fear","07":"disgust","08":"surprise"}
CREMA_EMO_MAP = {'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fear', 'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'}

def list_tess_files():
    rows = []
    for emo in TESS_EMOTIONS:
        for actor_folder in ["OAF", "YAF"]:
            patt = os.path.join(TESS_DIR, f"{actor_folder}_{emo}", "*.wav")
            rows.extend([{"dataset":"TESS","emotion":emo,"path":p} for p in glob.glob(patt)])
    return rows

def list_ravdess_files():
    rows = []
    for p in glob.glob(os.path.join(RAVDESS_DIR, "Actor_*", "*.wav")):
        base = os.path.basename(p).split('-')
        if len(base) >= 3:
            emo = RAVDESS_EMO_MAP.get(base[2])
            if emo: rows.append({"dataset":"RAVDESS","emotion":emo,"path":p})
    return rows

# ADDED FOR CREMA-D
def list_crema_files():
    rows = []
    for p in glob.glob(os.path.join(CREMA_AUDIO_DIR, "*.wav")):
        base = os.path.basename(p).split('_')
        if len(base) >= 3:
            emo = CREMA_EMO_MAP.get(base[2])
            if emo: rows.append({"dataset":"CREMA-D","emotion":emo,"path":p})
    return rows


if 'eda_flags' not in globals():
    # MODIFIED to include CREMA-D
    manifest_list = list_tess_files() + list_ravdess_files() + list_crema_files()
    manifest = pd.DataFrame(manifest_list)
    print(f"Building eda_flags from manifest of {len(manifest)} files...")

    rows = []
    dur_lo, dur_hi = DUR_RANGE_OK
    # The tqdm library adds a progress bar, which is helpful for long processes
    from tqdm.auto import tqdm
    for _, r in tqdm(manifest.iterrows(), total=len(manifest)):
        try:
            s = inspect_raw(r["path"], trim_db=TRIM_DB)
            s.update(r)
            s["needs_resample"] = (s["sr"] != TARGET_SR)
            s["needs_normalize"] = (s["peak_amp"] < PEAK_MIN_FOR_OK)
            s["needs_dc_remove"] = (abs(s["mean_amp"]) > ABS_MEAN_MAX_OK)
            s["needs_trim"] = (s["leading_silence_ms"] > SILENCE_MS_OK) or (s["trailing_silence_ms"] > SILENCE_MS_OK)
            s["has_clipping"] = (s["clipping_pct"] >= CLIP_PCT_OK)
            s["duration_out_of_range"] = not (dur_lo <= s["duration_sec"] <= dur_hi)
            rows.append(s)
        except Exception as e:
            print(f"Error processing {r['path']}: {e}")


    eda_flags = pd.DataFrame(rows)
    print("eda_flags built:", eda_flags.shape)
else:
    print("eda_flags already exists:", eda_flags.shape)



# 1. Define  harmonization map and final taxonomy
FINAL_TAXONOMY = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust', 'surprise']
HARMONIZE_MAP = {
    'fearful': 'fear',
    'pleasant_surprise': 'surprise',
    'ps': 'surprise',
    'calm': 'neutral', # The most important mapping
}

# 2. Apply the harmonization to the 'emotion' column
eda_flags['emotion'] = eda_flags['emotion'].replace(HARMONIZE_MAP)

# 3. Filter the DataFrame to keep only the final 7 emotions
#    and reset the index to prevent potential issues later.
before_count = len(eda_flags)
eda_flags = eda_flags[eda_flags['emotion'].isin(FINAL_TAXONOMY)].copy().reset_index(drop=True)

print(f"\nApplied harmonization: Kept {len(eda_flags)}/{before_count} files.")
print("Final unique emotions in eda_flags:", sorted(eda_flags['emotion'].unique()))

# Ensure the eda_flags DataFrame from the previous step exists
assert 'eda_flags' in globals(), "Please run the previous cell to build the 'eda_flags' DataFrame first."

# Define the flag columns we want to analyze
flag_cols = [
    "needs_resample",
    "needs_normalize",
    "needs_dc_remove",
    "needs_trim",
    "has_clipping",
    "duration_out_of_range",
]

# ---------- 1) Counts by dataset (table) ----------
print("--- DIAGNOSTICS SUMMARY TABLE ---")
by_dataset = (
    eda_flags.groupby("dataset")[flag_cols]
    .sum()
    .astype(int)
)
total_by_dataset = eda_flags.groupby("dataset")["path"].count().rename("total_files")
summary_table = by_dataset.join(total_by_dataset)
print(summary_table.to_string())


# ---------- 2) Grouped bar chart: Files needing each action (by dataset) ----------
datasets = summary_table.index.tolist()
n_datasets = len(datasets)
x = np.arange(len(flag_cols))
width = 0.8 / n_datasets  # Calculate bar width based on number of datasets

fig, ax = plt.subplots(figsize=(12, 6))

# Loop through datasets to plot bars side-by-side
for i, dataset in enumerate(datasets):
    offset = width * (i - (n_datasets - 1) / 2)
    values = summary_table.loc[dataset, flag_cols].values
    ax.bar(x + offset, values, width, label=dataset)

ax.set_xticks(x)
ax.set_xticklabels([
    "Resample", "Normalize", "DC Remove", "Trim Silence", "Has Clipping", "Duration Out-of-Range"
], rotation=15, ha="right")
ax.set_ylabel("Number of Files")
ax.set_title("Count of Files Requiring Each Cleaning Action by Dataset")
ax.legend()
ax.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()


# ---------- 3) Per-emotion heatmap view (counts) ----------
def per_emotion_matrix(df, dataset_name):
    df_ds = df[df["dataset"] == dataset_name]
    # Ensure 'emotion' column exists before grouping
    if 'emotion' not in df_ds.columns:
        print(f"Warning: 'emotion' column not found for {dataset_name}. Skipping heatmap.")
        return None
    mat = (
        df_ds.groupby("emotion")[flag_cols]
        .sum()
        .astype(int)
        .reindex(sorted(df_ds["emotion"].unique()))
    )
    return mat

for ds in summary_table.index:
    mat = per_emotion_matrix(eda_flags, ds)
    if mat is None or mat.empty:
        continue
    plt.figure(figsize=(10, 0.5 * len(mat)))
    sns.heatmap(mat, annot=True, fmt='d', cmap='viridis', linewidths=.5)
    plt.title(f"{ds}: Count of Files Needing Each Action (by Emotion)")
    plt.xlabel("Cleaning Action")
    plt.ylabel("Emotion")
    plt.xticks(ticks=np.arange(len(flag_cols)) + 0.5, labels=["Resample","Normalize","DC","Trim","Clip","Dur OOR"], rotation=0)
    plt.show()


# ---------- 4) Distributions of raw metrics to justify thresholds ----------
def hist_by_dataset(metric, bins=40, title=""):
    fig, ax = plt.subplots(figsize=(10,4))
    for ds, group in eda_flags.groupby("dataset"):
        ax.hist(group[metric].dropna(), bins=bins, alpha=0.6, label=ds)
    ax.set_title(title)
    ax.set_xlabel(metric.replace('_', ' ').title())
    ax.set_ylabel("Count")
    ax.legend()
    ax.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()

# Calculate total silence for a more informative histogram
if "silence_total_ms" not in eda_flags.columns:
    eda_flags["silence_total_ms"] = eda_flags["leading_silence_ms"] + eda_flags["trailing_silence_ms"]

hist_by_dataset("duration_sec", title="Duration Distribution by Dataset")
hist_by_dataset("rms_db",       title="Loudness (RMS in dB) Distribution by Dataset")
hist_by_dataset("silence_total_ms", title="Total Silence (ms) Distribution by Dataset")

"""# <p>**The Strategy: A Two-Phase Cleaning Approach**</p>
**`Phase 1: Minimal Standardization `**<br>
This phase applies safe, global transformations that make the audio files technically uniform without altering the core spoken content.

Actions: Resampling to a common sample rate, converting to mono, and normalizing the peak volume.

Why it's "Minimal" and "Safe": These steps standardize the "container" of the audio. They ensure every file uses the same measurement scale (sample rate) and has a similar overall volume. This process doesn't remove any part of the actual recording, like a word or a pause. It is a required prerequisite for almost any feature extraction method.

**`Phase 2: Content-Aware Cleaning `** <br>
This next phase involves actions that can change the content of the audio and must be handled more carefully.

Actions: Trimming leading and trailing silence.


Trimming silence is a powerful way to focus the model on the relevant speech. However, if your trimming threshold is too aggressive, you risk cutting off a quiet consonant at the beginning of a word or a soft breath at the end. This would fundamentally damage the data.

**`Phase 1: Minimal Standardization`**
This code block defines a core preprocessing function, preprocess_audio, that applies safe, global transformations to every audio file. It then demonstrates the function's effect on a single sample file, showing a "before and after" comparison.

`**Key Actions:**`

**Resample:** Converts all audio to a single sample rate (22050 Hz).

**Convert to Mono:** Merges stereo channels into a single channel.

**Normalize Amplitude:** Adjusts the volume so that the loudest point is at a consistent level (95% of the maximum possible). This ensures all files have a similar volume.
"""

# --- Configuration for Standardization ---
TARGET_SR = 22050
TARGET_AMP = 0.95 # Normalize audio to 95% of the maximum amplitude

# --- Redefining Helper Functions for Consistency ---
# We redefine these functions here to ensure they return the long, descriptive keys
# that the print_stats() function (from a previous cell) expects. This resolves the KeyError.

def audio_stats(y, sr, trim_db=20):
    """Calculates diagnostics and returns a dictionary with full-text keys."""
    y_mono = librosa.to_mono(y) if y.ndim == 2 else y
    dur = len(y_mono) / sr if sr else 0.0
    peak = float(np.max(np.abs(y_mono))) if y_mono.size else 0.0
    mean_amp = float(np.mean(y_mono)) if y_mono.size else 0.0
    rms = float(np.sqrt(np.mean(y_mono**2))) if y_mono.size else 0.0
    rms_db = 20 * np.log10(rms + 1e-12)
    _, idx = librosa.effects.trim(y_mono, top_db=trim_db)
    lead_ms = (idx[0] / sr) * 1000
    trail_ms = ((len(y_mono) - idx[1]) / sr) * 1000
    clip_pct = 100.0 * float(np.mean(np.abs(y_mono) >= 0.999)) if y_mono.size else 0.0
    return {
        "Sample rate (Hz)": sr,
        "Duration (sec)": round(dur, 3),
        "Peak amplitude": round(peak, 4),
        "RMS (dB)": round(rms_db, 2),
        "Mean amplitude": round(mean_amp, 4),
        "Leading silence (ms)": int(lead_ms),
        "Trailing silence (ms)": int(trail_ms),
        "Clipping (%)": round(clip_pct, 3),
        "_y": y_mono,
        "_sr": sr
    }

def inspect_raw(path, trim_db=20):
    """Loads a file as-is and returns its full diagnostic stats."""
    y, sr = librosa.load(path, sr=None, mono=False)
    return audio_stats(y, sr, trim_db=trim_db)


# --- Main Preprocessing Function ---
def preprocess_audio(path):
    """
    Loads an audio file and applies minimal standardization.
    """
    try:
        y, sr = librosa.load(path, sr=TARGET_SR, mono=True)
        max_amp = np.max(np.abs(y))
        if max_amp > 0:
            y = (y / max_amp) * TARGET_AMP
        return y, sr
    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None, None


# --- Demonstration: Before vs. After (with hardcoded path) ---

# MODIFIED: Path is now hardcoded to a specific RAVDESS file for a consistent demo.
# This ensures the output is the same every time the notebook is run.
demo_path = "/content/data/speech_emotion_ravdess_tess_crema/RAVDESS/Actor_01/03-01-01-01-01-01-01.wav"

# Safety check to ensure the file exists before proceeding
if os.path.exists(demo_path):
    print("="*80)
    print(f"PHASE 1: MINIMAL STANDARDIZATION, DEMONSTRATION on file: {os.path.basename(demo_path)}")
    print("="*80)

    # 1. Show the file BEFORE processing
    before_stats = inspect_raw(demo_path)
    print_stats(before_stats, title="BEFORE STANDARDIZATION", path_for_context=demo_path)

    # 2. Process the file
    processed_y, processed_sr = preprocess_audio(demo_path)

    # 3. Show the file AFTER processing
    after_stats = audio_stats(processed_y, processed_sr)
    print_stats(after_stats, title="AFTER STANDARDIZATION", path_for_context=demo_path)

    # Also, show the new waveform
    print("\nWaveform after standardization:")
    preview_waveform(processed_y, processed_sr)
else:
    print(f"ERROR: Demo file not found at the specified path: {demo_path}")
    print("Please ensure you have run the data sync step ('0B. SPEED-UP') correctly.")

# --- Configuration for Phase 2 ---
# The 'top_db' tells librosa how aggressively to trim. 20 is a safe, conservative value.
TRIM_DB = 20

# --- Phase 2 Cleaning Function ---
def trim_silence(y, top_db=TRIM_DB):
    """
    Trims leading and trailing silence from an audio signal.
    This is a content-aware operation.
    """
    y_trimmed, _ = librosa.effects.trim(y, top_db=top_db)
    return y_trimmed

# --- Demonstration: The Effect of Trimming Silence ---
# We will use the same hardcoded RAVDESS file for a consistent demonstration.
demo_path = "/content/data/speech_emotion_ravdess_tess_crema/RAVDESS/Actor_01/03-01-01-01-01-01-01.wav"

# Safety check
if os.path.exists(demo_path):

    # First, get the standardized audio (the output of Phase 1) to use as our starting point.
    # This assumes the 'preprocess_audio' function from the previous cell is still in memory.
    standardized_y, sr = preprocess_audio(demo_path)

    # --- BEFORE running Phase 2 ---
    stats_before_trim = audio_stats(standardized_y, sr)
    print("="*80)
    print("DEMONSTRATION: APPLYING PHASE 2 (TRIMMING SILENCE)")
    print("="*80)
    print("\n--- BEFORE TRIMMING (but after standardization) ---")
    print_stats(stats_before_trim, path_for_context=demo_path)

    # --- RUN Phase 2 ---
    # Now, apply the trimming function to the standardized audio
    trimmed_y = trim_silence(standardized_y)

    # --- AFTER running Phase 2 ---
    stats_after_trim = audio_stats(trimmed_y, sr)
    print("\n--- AFTER TRIMMING ---")
    print_stats(stats_after_trim, path_for_context=demo_path)

    # --- VISUAL COMPARISON ---
    fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(12, 6))
    librosa.display.waveshow(standardized_y, sr=sr, ax=ax[0])
    ax[0].set(title='Waveform Before Trimming')
    ax[0].label_outer()
    librosa.display.waveshow(trimmed_y, sr=sr, ax=ax[1])
    ax[1].set(title='Waveform After Trimming')
    plt.show()

else:
    print(f"ERROR: Demo file not found at the specified path: {demo_path}")

"""**`Full Pipeline - Cleaning, Feature Extraction, and Caching`**<br>
This code creates the final processing pipeline. For each audio file, it will:

Apply Phase 1 (resample, mono, normalize).

Apply Phase 2 (conditionally trim silence).

Convert the clean audio into a log-mel spectrogram (a standard "image" of the sound).

Save the resulting spectrogram to a new cache directory for fast access during model training.
"""

# --- Configuration ---
TARGET_SR = 22050
TARGET_AMP = 0.95
TRIM_DB = 20

# Librosa feature extraction parameters
N_FFT = 2048
HOP_LENGTH = 512
N_MELS = 128

# --- Feature Cache Directory ---
FEATURES_DIR = os.path.join(LOCAL_ROOT, "features_log_mel")
os.makedirs(FEATURES_DIR, exist_ok=True)
print(f"Final features will be saved to: {FEATURES_DIR}")


# --- All-in-One Processing Function ---

def clean_and_extract_features(row):
    """
    Takes a row from eda_flags, runs the full cleaning pipeline,
    extracts log-mel features, and saves them to a file.
    """
    original_path = row['path']
    needs_trim = row['needs_trim']

    # Define the final path for the cached .npy feature file
    base_name = os.path.basename(original_path).replace('.wav', '.npy')
    feature_path = os.path.join(FEATURES_DIR, base_name)

    # If the feature file already exists, skip processing.
    if os.path.exists(feature_path):
        return feature_path

    try:
        # Phase 1: Minimal Standardization
        y, sr = librosa.load(original_path, sr=TARGET_SR, mono=True)
        y = librosa.util.normalize(y, norm=np.inf) * TARGET_AMP

        # Phase 2: Content-Aware Cleaning (Conditional)
        if needs_trim:
            y, _ = librosa.effects.trim(y, top_db=TRIM_DB)
            # Re-normalize after trimming
            y = librosa.util.normalize(y, norm=np.inf) * TARGET_AMP

        # Step 5: Feature Extraction
        mel_spectrogram = librosa.feature.melspectrogram(
            y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS
        )
        # Convert to decibels (log scale)
        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)

        # Save the final feature array
        np.save(feature_path, log_mel_spectrogram)
        return feature_path

    except Exception as e:
        print(f"Error processing {original_path}: {e}")
        return None

# --- Batch Processing ---
print(f"\nProcessing {len(eda_flags)} files. This will create the final features for modeling.")
tqdm.pandas(desc="Creating Features")
eda_flags['feature_path'] = eda_flags.progress_apply(clean_and_extract_features, axis=1)

print("\n--- Feature Extraction Complete ---")
processed_count = eda_flags['feature_path'].notna().sum()
print(f"Successfully created and cached features for {processed_count} / {len(eda_flags)} files.")

# --- Verification Step ---
print("\n--- Verifying a Sample Feature File ---")
# Load one of the feature files we just created
try:
    sample_feature_path = eda_flags['feature_path'].dropna().iloc[0]
    sample_feature = np.load(sample_feature_path)

    print(f"Loaded sample feature from: {os.path.basename(sample_feature_path)}")
    print(f"Feature shape: {sample_feature.shape} (mel_bins, time_frames)")

    # Display the spectrogram
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(sample_feature, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Log-Mel Spectrogram of a Cleaned Audio File')
    plt.tight_layout()
    plt.show()

except (IndexError, NameError):
    print("Could not load a sample feature to display.")

"""EDA on Extracted Features (Log-Mel Spectrograms)"""

# Ensure the eda_flags DataFrame from the previous step exists
assert 'eda_flags' in globals() and 'feature_path' in eda_flags.columns, \
    "Please run the previous cell to create the 'eda_flags' DataFrame with 'feature_path'."

# --- Part 1: Visualize a Fixed Sample from Each Emotion ---

# Get a list of unique emotions that were successfully processed
valid_emotions = sorted(eda_flags.dropna(subset=['feature_path'])['emotion'].unique())
n_emotions = len(valid_emotions)

print("--- EDA Part 1: Sample Spectrograms per Emotion ---")

# Create a grid of plots
fig, axes = plt.subplots(nrows=n_emotions, ncols=1, figsize=(8, n_emotions * 3))
fig.suptitle("Fixed Sample Spectrograms per Emotion", fontsize=16)

for i, emo in enumerate(valid_emotions):
    # --- MODIFIED ---
    # Replaced .sample(1) with .iloc[0] to deterministically select the FIRST file
    # for each emotion. This ensures the same plot is generated every time.
    sample_row = eda_flags[eda_flags['emotion'] == emo].dropna(subset=['feature_path']).iloc[0]
    feature = np.load(sample_row['feature_path'])

    ax = axes[i]
    img = librosa.display.specshow(feature, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel', ax=ax)
    ax.set_title(f"Emotion: {emo.capitalize()}")
    fig.colorbar(img, ax=ax, format='%+2.0f dB')

plt.tight_layout(rect=[0, 0.03, 1, 0.98])
plt.show()


# --- Part 2: Visualize the AVERAGE Spectrogram for Each Emotion ---
# This part is already deterministic and does not need to be changed.

print("\n--- EDA Part 2: Average Spectrograms per Emotion ---")
print("Calculating averages... this may take a moment.")

# First, determine a common length to resize all spectrograms to (e.g., the median length)
all_lengths = [np.load(p).shape[1] for p in eda_flags['feature_path'].dropna()]
target_len = int(np.median(all_lengths))

def pad_or_truncate(array, length):
    """Pads or truncates a 2D array along the time axis (axis=1)."""
    if array.shape[1] > length:
        return array[:, :length]
    else:
        padding = length - array.shape[1]
        return np.pad(array, ((0, 0), (0, padding)), mode='constant', constant_values=array.min())

# Create a new grid for the average plots
fig, axes = plt.subplots(nrows=n_emotions, ncols=1, figsize=(8, n_emotions * 3))
fig.suptitle("Average Spectrograms per Emotion", fontsize=16)

for i, emo in enumerate(valid_emotions):
    # Get all feature paths for the current emotion
    emotion_paths = eda_flags[eda_flags['emotion'] == emo]['feature_path'].dropna().tolist()

    # Load, resize, and stack all spectrograms for this emotion
    resized_spectrograms = [pad_or_truncate(np.load(p), target_len) for p in emotion_paths]

    if resized_spectrograms:
        # Calculate the mean across all samples
        average_spectrogram = np.mean(np.stack(resized_spectrograms), axis=0)

        ax = axes[i]
        img = librosa.display.specshow(average_spectrogram, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel', ax=ax)
        ax.set_title(f"Average for: {emo.capitalize()}")
        fig.colorbar(img, ax=ax, format='%+2.0f dB')

plt.tight_layout(rect=[0, 0.03, 1, 0.98])
plt.show()

"""**Creating Speaker-Disjoint Data Splits**

This is one of the most critical steps in building a reliable audio emotion recognition model. <br>

**Why This Step is Crucial**
To ensure that the model is truly learning to identify emotions and not just memorizing the voices of specific speakers, the "speaker-disjoint" (or speaker-independent) splits was created. This means that a single speaker can only appear in one of the sets (training, validation, or testing).

**This script will:**

Extract a unique Speaker ID for every audio file.

Split the list of speakers into train, validation, and test groups.

Create the final dataframes based on this speaker split.

Verify that there is no speaker overlap between the sets.
"""

# Ensure the eda_flags DataFrame from the previous step exists
assert 'eda_flags' in globals() and 'feature_path' in eda_flags.columns, \
    "Please run the previous cell to create the 'eda_flags' DataFrame."

# --- Step 1: Extract Speaker ID for Each File ---

def get_speaker_id(row):
    """Extracts the speaker ID based on the dataset and file path."""
    dataset = row['dataset']
    path = row['path']

    if dataset == 'RAVDESS':
        # Path: .../RAVDESS/Actor_14/03-01-02-01-02-01-14.wav
        # The folder name 'Actor_14' is a reliable ID.
        match = re.search(r'Actor_(\d+)', path)
        if match:
            return f"ravdess_{match.group(1)}"

    elif dataset == 'TESS':
        # Path: .../TESS/OAF_angry/OAF_back_angry.wav
        # The two speakers are 'OAF' (Older) and 'YAF' (Younger).
        if 'OAF_' in path:
            return 'tess_OAF'
        elif 'YAF_' in path:
            return 'tess_YAF'

    elif dataset == 'CREMA-D':
        # Filename: 1076_MTI_SAD_XX.wav
        # The first part of the filename is the speaker ID.
        filename = os.path.basename(path)
        return f"crema_{filename.split('_')[0]}"

    return None

# Apply the function to create a new 'speaker_id' column
eda_flags['speaker_id'] = eda_flags.apply(get_speaker_id, axis=1)

print("--- Speaker ID Extraction Complete ---")
print("Sample of Speaker IDs:")
print(eda_flags[['dataset', 'speaker_id']].sample(5))


# --- Step 2: Split Speakers into Train, Validation, and Test Sets ---

# Get a unique, sorted list of all speaker IDs
all_speakers = sorted(eda_flags['speaker_id'].unique())
np.random.seed(42) # for reproducibility
np.random.shuffle(all_speakers)

# Define split proportions
train_split = 0.8
val_split = 0.1
# Test split will be the remainder

# Split the list of speakers
train_speakers, val_speakers, test_speakers = np.split(all_speakers, [
    int(len(all_speakers) * train_split),
    int(len(all_speakers) * (train_split + val_split))
])

print(f"\n--- Speaker Split ---")
print(f"Total Speakers: {len(all_speakers)}")
print(f"Training Speakers: {len(train_speakers)}")
print(f"Validation Speakers: {len(val_speakers)}")
print(f"Test Speakers: {len(test_speakers)}")


# --- Step 3: Create Final DataFrames ---

train_df = eda_flags[eda_flags['speaker_id'].isin(train_speakers)].copy()
val_df = eda_flags[eda_flags['speaker_id'].isin(val_speakers)].copy()
test_df = eda_flags[eda_flags['speaker_id'].isin(test_speakers)].copy()

print("\n--- Final DataFrame Sizes ---")
print(f"Training set size:   {len(train_df)} files")
print(f"Validation set size: {len(val_df)} files")
print(f"Test set size:       {len(test_df)} files")


# --- Step 4: Verification ---

# Check for any speaker overlap between sets
train_speaker_set = set(train_df['speaker_id'].unique())
val_speaker_set = set(val_df['speaker_id'].unique())
test_speaker_set = set(test_df['speaker_id'].unique())

assert len(train_speaker_set.intersection(val_speaker_set)) == 0, "Overlap found between train and val speakers!"
assert len(train_speaker_set.intersection(test_speaker_set)) == 0, "Overlap found between train and test speakers!"
assert len(val_speaker_set.intersection(test_speaker_set)) == 0, "Overlap found between val and test speakers!"

print("\n✅ Verification Successful: No speaker overlap found between sets.")


# --- Step 5: Review Emotion Distribution ---

print("\n--- Emotion Distribution in Each Set ---")
print("\nTraining Set:")
print(train_df['emotion'].value_counts().sort_index())
print("\nValidation Set:")
print(val_df['emotion'].value_counts().sort_index())
print("\nTest Set:")
print(test_df['emotion'].value_counts().sort_index())

assert 'eda_flags' in globals(), "Please run the previous cells to create the 'eda_flags' DataFrame."

print("--- Visualizing Overall Emotion Distribution ---")

# Calculate the counts for each emotion in the full dataset
emotion_counts = eda_flags['emotion'].value_counts()

# Create the bar plot
plt.figure(figsize=(12, 7))
sns.barplot(x=emotion_counts.index, y=emotion_counts.values, palette="viridis")

plt.title('Class Imbalances in the Combined Dataset', fontsize=16)
plt.ylabel('Number of Files', fontsize=12)
plt.xlabel('Emotion', fontsize=12)
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

"""# Data Preparation (Part 1): Label Encoding & Finding Target Length
This first block handles the preliminary setup. It encodes the text labels into numbers and then calculates the single, uniform length that all feature vectors will be resized to.
"""

from sklearn.preprocessing import LabelEncoder

# --- Step 1: Encode Emotion Labels ---
# This step converts the string labels ('happy', 'sad') into integers.

# Ensure the DataFrames from the splitting step exist
assert 'train_df' in globals(), "Please run the data splitting cell first."

# We create the encoder and fit it ONLY on the training data to avoid data leakage.
label_encoder = LabelEncoder()
train_df['emotion_encoded'] = label_encoder.fit_transform(train_df['emotion'])
val_df['emotion_encoded'] = label_encoder.transform(val_df['emotion'])
test_df['emotion_encoded'] = label_encoder.transform(test_df['emotion'])

print("--- Label Encoding Complete ---")
# Show the mapping from text to number
for i, label in enumerate(label_encoder.classes_):
    print(f"{label}: {i}")

# --- Step 2: Pre-calculation to Find a Uniform Feature Length ---
print("\n--- Determining a uniform feature length... ---")
# We'll check the length of every feature to find the median, which is robust to outliers.
all_feature_paths = eda_flags['feature_path'].dropna().tolist()
all_lengths = [np.load(p).shape[1] for p in tqdm(all_feature_paths, desc="Checking feature lengths")]

# This is the target number of time_frames for every spectrogram
TARGET_LEN = int(np.median(all_lengths))
# N_MELS was defined in the feature extraction step (e.g., 128)
UNIFORM_FEATURE_SIZE = TARGET_LEN * N_MELS

print(f"\nTarget number of time frames set to: {TARGET_LEN}")
print(f"All feature vectors will be resized to a uniform size of {UNIFORM_FEATURE_SIZE} features.")

"""#Data Preparation (Part 2): Audio Augmentation for Training Only + Padding & Truncating Features
Now that TARGET_LEN has been calculated, this block defines a function that loads each saved spectrogram (.npy),applies audio augmentation to training data only, and then pads or truncates it so every spectrogram has the same time length.

**How it works:**

1.   Load the spectrogram from disk.
2.   If training mode is enabled (train_mode=True):


*   Apply spectrogram augmentation (e.g., time masking, frequency masking) to introduce controlled variability in the training data, helping the model generalize better and reduce overfitting.
*   Validation and test data remain clean (no augmentation) to ensure evaluation is fair and reflects real-world performance.


3.   Pad or truncate the spectrogram along the time axis to exactly TARGET_LEN:


*   Truncate if the spectrogram is longer.
*   Pad with the minimum value (representing silence) if it’s shorter.

4.   Flatten the 2D spectrogram into a 1D feature vector for model input.
5.   Collect these feature vectors for:

*   X_train → with augmentation
*   X_val and X_test → without augmentation

**Why this step is important:**

Uniform length ensures that all samples have the same feature size, which is required for most machine learning models.

Augmentation on training only improves robustness without contaminating validation or test performance measurements.

Keeping validation/test clean avoids artificial score inflation and ensures metrics reflect performance on real, unseen data.
"""

# Ensure the TARGET_LEN variable from the previous cell exists
assert 'TARGET_LEN' in globals(), "Please run the previous cell to calculate TARGET_LEN first."

# --- Augmentation functions ---


AUG_P = 0.5                # chance to apply augmentation
PITCH_STEPS = (-1.0, 1.0)  # semitone range for pitch shift
STRETCH_RANGE = (0.95, 1.05)
SNR_DB = (20, 30)          # noise strength

def maybe_augment_wave(y, sr, train_mode=False):
    if not train_mode or random.random() > AUG_P:
        return y
    choice = random.choice(["pitch", "stretch", "noise", "none", "none"])  # bias to do nothing
    if choice == "pitch":
        steps = random.uniform(*PITCH_STEPS)
        y = librosa.effects.pitch_shift(y, sr, n_steps=steps)
    elif choice == "stretch":
        rate = random.uniform(*STRETCH_RANGE)
        y = librosa.effects.time_stretch(y, rate)
    elif choice == "noise":
        snr = random.uniform(*SNR_DB)
        sig_pow = np.mean(y**2) + 1e-12
        noise_pow = sig_pow / (10**(snr/10))
        y = y + np.random.normal(0, np.sqrt(noise_pow), size=y.shape)
    return y

def spec_augment(S, max_time_mask=0.1, max_freq_mask=0.1, n_masks=1, train_mode=False):
    if not train_mode:
        return S
    S = S.copy()
    F, T = S.shape
    for _ in range(n_masks):
        # time mask
        t = int(T * max_time_mask)
        if t > 0:
            t0 = np.random.randint(0, max(1, T - t))
            S[:, t0:t0+t] = S[:, t0:t0+t].mean()
        # freq mask
        f = int(F * max_freq_mask)
        if f > 0:
            f0 = np.random.randint(0, max(1, F - f))
            S[f0:f0+f, :] = S[f0:f0+f, :].mean()
    return S

# --- Function to Load, Pad/Truncate, Apply Augmentation, and Flatten Features ---
from tqdm import tqdm

def load_and_process_features(df, target_len, train_mode=False):
    """
    Loads spectrograms, applies augmentation if train_mode=True, resizes to a target length,
    and flattens them into vectors.
    """
    features = []
    for feature_path in tqdm(df['feature_path'], desc="Loading and Resizing Features"):
        spectrogram = np.load(feature_path)

        # Apply spectrogram augmentation for training
        if train_mode:
            spectrogram = spec_augment(spectrogram, train_mode=True)

        # Pad or truncate the spectrogram on the time axis (axis=1)
        if spectrogram.shape[1] > target_len:
            spectrogram = spectrogram[:, :target_len]  # Truncate
        else:
            padding = target_len - spectrogram.shape[1]
            spectrogram = np.pad(
                spectrogram,
                ((0, 0), (0, padding)),
                mode='constant',
                constant_values=spectrogram.min()
            )

        features.append(spectrogram.flatten())

    return np.array(features)

# --- Load and Process the Data Splits ---
X_train = load_and_process_features(train_df, TARGET_LEN, train_mode=True)
X_val   = load_and_process_features(val_df, TARGET_LEN, train_mode=False)
X_test  = load_and_process_features(test_df, TARGET_LEN, train_mode=False)

# Get the corresponding encoded labels from the DataFrames
y_train = train_df['emotion_encoded'].values
y_val = val_df['emotion_encoded'].values
y_test = test_df['emotion_encoded'].values

# --- Verification ---
print("\n--- Data Preparation Complete ---")
print(f"Shape of X_train (features): {X_train.shape}")
print(f"Shape of y_train (labels):   {y_train.shape}")
print(f"\nShape of X_val (features):   {X_val.shape}")
print(f"Shape of y_val (labels):     {y_val.shape}")
print(f"\nShape of X_test (features):  {X_test.shape}")
print(f"Shape of y_test (labels):    {y_test.shape}")



"""**Calculating class weights**<br>
Augmentation and padding/truncating happen to the features, while class weight calculation is based on the label distribution in the original train_df['emotion'].

"""

from sklearn.utils.class_weight import compute_class_weight

# Ensure the train_df DataFrame from the splitting step exists
assert 'train_df' in globals(), "Please run the data splitting cell first to create 'train_df'."

print("--- Calculating Class Weights ---")

# Get the list of unique classes from the training data
class_labels = np.unique(train_df['emotion'])

# Calculate the weights
class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=class_labels,
    y=train_df['emotion']
)


# Create a dictionary mapping the ENCODED INTEGER LABELS (0, 1, 2...) to the weights.
# The label_encoder.transform() correctly converts string labels to their integer representation.
class_weights = dict(zip(label_encoder.transform(class_labels), class_weights_array))

print("Successfully calculated class weights with INTEGER keys:\n")
# We use label_encoder.classes_ to show the original emotion name for readability
for i, label in enumerate(label_encoder.classes_):
    print(f"{label:<10} (label {i}): {class_weights[i]:.4f}")

"""Scaling the Data"""

from sklearn.preprocessing import StandardScaler


# Ensure the feature sets (X_train, X_val, X_test) from the previous step exist
assert 'X_train' in globals(), "Please run the previous data preparation cell first."

print("--- Scaling Features ---")

# 1. Initialize the Scaler
scaler = StandardScaler()

# 2. Fit on the training data ONLY and transform it
X_train_scaled = scaler.fit_transform(X_train)

# 3. Transform the validation and test data using the SAME fitted scaler
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# --- Verification Step (with improved formatting) ---
print("Scaling complete.")
print(f"\nOriginal X_train shape: {X_train.shape}")
print(f"Scaled X_train shape:   {X_train_scaled.shape}")

# Create a DataFrame to display the stats in a clean table
verification_data = {
    'Train Mean': np.mean(X_train_scaled[:, :5], axis=0),
    'Train Std Dev': np.std(X_train_scaled[:, :5], axis=0),
    'Test Mean': np.mean(X_test_scaled[:, :5], axis=0),
    'Test Std Dev': np.std(X_test_scaled[:, :5], axis=0),
}
verification_df = pd.DataFrame(
    verification_data,
    index=[f'Feature {i}' for i in range(5)]
)

# Format the numbers in the table for better readability
pd.options.display.float_format = '{:.6f}'.format

print("\n--- Verification of Scaled Features (First 5 Features) ---")
display(verification_df)

# Reset float format to default if you want
# pd.options.display.float_format = None

"""**`Training Baseline Machine Learning Models`**
This script will train the first three required traditional models: Logistic Regression, Random Forest, and LGBMClassifier.

It uses the scaled data (X_train_scaled, y_train) and, most importantly, passes the class_weights dictionary to each model to handle the data imbalance. Performance is measured on the validation set.
"""

assert 'X_train_scaled' in globals() and 'class_weights' in globals()

# Quiet LightGBM logs + warnings
warnings.filterwarnings("ignore", module="lightgbm")
os.environ["LIBOMP_WARN"] = "0"

CPU = os.cpu_count() or 4
N_WORKERS = max(1, CPU - 1)   # keep 1 core free

# --- small stratified subset for fast iteration ---
MAX_SAMPLES = 1500  # tweak as needed
sss = StratifiedShuffleSplit(n_splits=1, train_size=min(MAX_SAMPLES, len(y_train)), random_state=42)
idx_small, _ = next(sss.split(X_train_scaled, y_train))
Xtr_small, ytr_small = X_train_scaled[idx_small], y_train[idx_small]
print(f"Subset for fast baselines: {len(ytr_small)} rows")

# --- models (parallelized where it matters) ---
models = {
    "Logistic Regression": LogisticRegression(
        solver="saga", C=1.0, max_iter=2000, class_weight=class_weights
    ),
    "Random Forest": RandomForestClassifier(
        n_estimators=600, max_depth=None, n_jobs=N_WORKERS,
        class_weight=class_weights, random_state=42
    ),
    "LightGBM": LGBMClassifier(
        objective="multiclass", class_weight=class_weights,
        n_estimators=800, learning_rate=0.1,
        subsample=0.8, colsample_bytree=0.8,
        n_jobs=N_WORKERS, random_state=42,
        verbose=-1  # <— suppress LightGBM training spam
    ),
}

def _auc_ovr(y_true, proba):
    try:
        return roc_auc_score(y_true, proba, multi_class='ovr')
    except Exception:
        return np.nan

def _diagnose(acc_tr, acc_va, f1_tr, f1_va, tol=0.05):
    gap_acc = (acc_tr - acc_va)
    gap_f1  = (f1_tr - f1_va)
    if (acc_tr > 0.90 and gap_acc >= tol) or (f1_tr > 0.90 and gap_f1 >= tol) or (gap_acc >= tol and gap_f1 >= tol):
        status = "Overfitting"
    elif (acc_tr < 0.70 and acc_va < 0.70) and (f1_tr < 0.70 and f1_va < 0.70):
        status = "Underfitting"
    else:
        status = "Reasonable fit"
    return status, gap_acc * 100.0, gap_f1 * 100.0

pd.options.display.float_format = '{:.4f}'.format

for name, model in models.items():
    print("="*80, f"\nTraining {name} on subset...")
    model.fit(Xtr_small, ytr_small)

    # --- preds + probas ---
    y_tr = model.predict(Xtr_small)
    y_va = model.predict(X_val_scaled)
    proba_fn = getattr(model, "predict_proba", None)
    auc_tr = _auc_ovr(ytr_small, proba_fn(Xtr_small)) if proba_fn else np.nan
    auc_va = _auc_ovr(y_val,      proba_fn(X_val_scaled)) if proba_fn else np.nan

    # --- metrics ---
    acc_tr = accuracy_score(ytr_small, y_tr)
    acc_va = accuracy_score(y_val, y_va)
    f1m_tr = f1_score(ytr_small, y_tr, average='macro')
    f1m_va = f1_score(y_val, y_va, average='macro')
    bal_tr = balanced_accuracy_score(ytr_small, y_tr)
    bal_va = balanced_accuracy_score(y_val, y_va)
    kap_tr = cohen_kappa_score(ytr_small, y_tr)
    kap_va = cohen_kappa_score(y_val, y_va)
    mcc_tr = matthews_corrcoef(ytr_small, y_tr)
    mcc_va = matthews_corrcoef(y_val, y_va)

    # === NEW: compact summary like your SVM printout (kept + expanded) ===
    print(f"Training Accuracy of {name}: {acc_tr*100:.2f}%")
    print(f"Accuracy (Validation) score of {name}: {acc_va*100:.2f}%")
    print(f"F1-macro (Train): {f1m_tr*100:.2f}%")
    print(f"F1-macro (Validation): {f1m_va*100:.2f}%")
    print(f"AUC (OvR) - Train: {auc_tr:.6f} | Val: {auc_va:.6f}")

    status, gap_acc_pp, gap_f1_pp = _diagnose(acc_tr, acc_va, f1m_tr, f1m_va)
    print(f"Fit diagnosis: {status} (Train–Val gaps → Accuracy: {gap_acc_pp:.2f} pp, F1: {gap_f1_pp:.2f} pp)")

    # === keep your existing full table ===
    dfm = pd.DataFrame({
        "Train":[acc_tr, f1m_tr, bal_tr, auc_tr, kap_tr, mcc_tr],
        "Val":[acc_va, f1m_va, bal_va, auc_va, kap_va, mcc_va]
    }, index=["Accuracy","F1 (macro)","Balanced Acc","ROC AUC (OvR)","Kappa","MCC"])
    display(dfm)

    # === keep your classification report ===
    print("\nValidation report:\n", classification_report(y_val, y_va, target_names=label_encoder.classes_))

    # === NEW: confusion matrices (raw + row-normalized) ===
    cm = confusion_matrix(y_val, y_va, labels=range(len(label_encoder.classes_)))
    cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)
    print("\nConfusion matrix (counts):")
    display(cm_df)

    with np.errstate(invalid='ignore'):
        cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)
    cmn_df = pd.DataFrame(np.nan_to_num(cm_norm), index=label_encoder.classes_, columns=label_encoder.classes_)
    print("Confusion matrix (row-normalized):")
    display(cmn_df.round(3))

"""Hyper parameter Tuning on LightGBM"""

# ===============================
# Anti-overfit refit (lean + fast)
# ===============================
import os, warnings
import numpy as np, pandas as pd
from IPython.display import display
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import (
    accuracy_score, f1_score, balanced_accuracy_score,
    cohen_kappa_score, matthews_corrcoef, classification_report
)
# Import the library and the classifier separately
from lightgbm import LGBMClassifier
import lightgbm as lgb

warnings.filterwarnings("ignore", module="lightgbm")

# --- Preconditions (fail fast) ---
assert 'y_train' in globals() and 'y_val' in globals(), "Run label-encoding/split cells."
assert 'class_weights' in globals(), "Run class-weights cell."
assert 'label_encoder' in globals(), "Missing label_encoder."
assert ('X_train_scaled' in globals() and 'X_val_scaled' in globals()) or \
       ('X_train_pca'   in globals() and 'X_val_pca'   in globals()), "Missing features."

# Prefer PCA-reduced features if present (faster, less overfit)
Xtr_full = globals().get('X_train_pca', X_train_scaled)
Xva_full = globals().get('X_val_pca',   X_val_scaled)

# ---- subset helper: use all rows unless very large ----
def stratified_cap_indices(X, y, cap=8000, seed=1337):
    n = len(y)
    if n <= cap:
        return np.arange(n)
    sss = StratifiedShuffleSplit(n_splits=1, train_size=cap, random_state=seed)
    idx, _ = next(sss.split(X, y))
    return idx

IDX = stratified_cap_indices(Xtr_full, y_train, cap=8000)
Xtr, ytr = Xtr_full[IDX].astype(np.float32), y_train[IDX]
Xva       = Xva_full.astype(np.float32); yva = y_val

def report(name, model, Xtr, ytr, Xva, yva, verbose=False):
    y_tr = model.predict(Xtr)
    y_va = model.predict(Xva)
    acc_tr = accuracy_score(ytr, y_tr); acc_va = accuracy_score(yva, y_va)
    f1_tr  = f1_score(ytr, y_tr, average='macro'); f1_va = f1_score(yva, y_va, average='macro')
    bal_tr = balanced_accuracy_score(ytr, y_tr);   bal_va = balanced_accuracy_score(yva, y_va)
    kap_tr = cohen_kappa_score(ytr, y_tr);         kap_va = cohen_kappa_score(yva, y_va)
    mcc_tr = matthews_corrcoef(ytr, y_tr);         mcc_va = matthews_corrcoef(yva, y_va)

    print("="*80, f"\n{name}")
    print(f"Train Acc/F1: {acc_tr*100:.2f}% / {f1_tr*100:.2f}%")
    print(f"Val   Acc/F1: {acc_va*100:.2f}% / {f1_va*100:.2f}%")
    print(f"Balanced Acc (Val): {bal_va*100:.2f}%")

    # Overfit flag
    gap_acc = acc_tr - acc_va
    gap_f1  = f1_tr  - f1_va
    status = "Overfitting" if (acc_tr>0.90 and gap_acc>=0.05) or (f1_tr>0.90 and gap_f1>=0.05) or (gap_acc>=0.05 and gap_f1>=0.05) else \
             "Underfitting" if (acc_tr<0.70 and acc_va<0.70 and f1_tr<0.70 and f1_va<0.70) else "Reasonable fit"
    print(f"Fit diagnosis: {status} (ΔAcc={gap_acc*100:.2f} pp, ΔF1={gap_f1*100:.2f} pp)")

    if verbose:
        print("\nValidation report:\n", classification_report(yva, y_va, target_names=label_encoder.classes_))
    return status

# ---- LightGBM (regularized + early stopping) ----
# Renamed variable to avoid conflict with the library import
lgbm_model = LGBMClassifier(
    objective="multiclass", class_weight=class_weights, num_leaves=31,
    min_child_samples=100, max_depth=-1, n_estimators=5000, learning_rate=0.06,
    feature_fraction=0.70, bagging_fraction=0.70, bagging_freq=1,
    lambda_l1=0.0, lambda_l2=10.0, min_split_gain=0.0, max_bin=63,
    n_jobs=max(1, (os.cpu_count() or 4) - 1), random_state=42, verbose=-1
)

# CORRECTED a`callbacks` to use the imported library `lgb`
lgbm_model.fit(
    Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
)
status = report("LightGBM (regularized + ES)", lgbm_model, Xtr, ytr, Xva, yva, verbose=True)

# ---- Auto-tighten once if still overfitting ----
if status == "Overfitting":
    print("\n[Auto-tighten] Reducing leaves, raising min_child_samples, increasing L2, shrinking subsample...")
    lgbm_model.set_params(
        num_leaves=max(15, lgbm_model.get_params()['num_leaves']//2),
        min_child_samples=min(300, lgbm_model.get_params()['min_child_samples']*2),
        lambda_l2=max(20.0, lgbm_model.get_params()['lambda_l2']*2),
        feature_fraction=max(0.5, lgbm_model.get_params()['feature_fraction'] - 0.1),
        bagging_fraction=max(0.5, lgbm_model.get_params()['bagging_fraction'] - 0.1),
    )
    # CORRECTED `callbacks` here as well
    lgbm_model.fit(
        Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
    )
    report("LightGBM (auto-tightened)", lgbm_model, Xtr, ytr, Xva, yva, verbose=True)

# --- OPTIONAL quick baselines (off by default) ---
RUN_LR = False
RUN_RF = False
if RUN_LR:
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression(solver="saga", penalty="elasticnet", l1_ratio=0.1,
                            C=0.25, max_iter=2000, class_weight=class_weights, n_jobs=max(1,(os.cpu_count() or 4)-1))
    lr.fit(Xtr, ytr); report("Logistic Regression (elastic net)", lr, Xtr, ytr, Xva, yva)
if RUN_RF:
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier(
        n_estimators=400, max_depth=16, min_samples_leaf=4, min_samples_split=12,
        max_features="sqrt", class_weight=class_weights, max_samples=0.8,
        n_jobs=max(1,(os.cpu_count() or 4)-1), random_state=42
    )
    rf.fit(Xtr, ytr); report("Random Forest (depth-capped + subsample)", rf, Xtr, ytr, Xva, yva)

"""Reducing Feature Dimensionality with PCA"""

# =================================================================
# JOURNEY STEP 2: Principal Component Analysis (PCA)
# =================================================================
from sklearn.decomposition import PCA


print("--- Applying PCA to reduce feature dimensions ---")

# --- Preconditions ---
assert 'X_train_scaled' in globals(), "Please run the data scaling cell first."

# Initialize PCA to keep components that explain 95% of the variance.
# This is a robust way to let the data decide the optimal number of dimensions.
pca = PCA(n_components=0.95, random_state=42)

# 1. Fit PCA on the training data ONLY to learn the components.
# 2. Transform the train, validation, and test sets using the learned components.
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)

# --- Verification ---
print("\n--- PCA Complete ---")
print(f"Original number of features: {X_train_scaled.shape[1]}")
print(f"PCA reduced features to:     {pca.n_components_}")
print(f"This explains {pca.explained_variance_ratio_.sum()*100:.2f}% of the original variance.")
print(f"\nNew shape of X_train_pca: {X_train_pca.shape}")
print(f"New shape of X_val_pca:   {X_val_pca.shape}")

"""Re training the model on PCA reduced features to lessen overfitting and improve the model"""

# =================================================================
# JOURNEY STEP 3: Training on PCA-Reduced Features
# =================================================================

print("--- Training LightGBM on PCA-reduced features ---")

# --- Preconditions ---
# This block will now find and use the `_pca` variables you just created.
assert 'X_train_pca' in globals(), "Please run the PCA cell first."
Xtr_full = X_train_pca
Xva_full = X_val_pca

# The subsetting and reporting functions are the same
IDX = stratified_cap_indices(Xtr_full, y_train, cap=8000)
Xtr, ytr = Xtr_full[IDX].astype(np.float32), y_train[IDX]
Xva       = Xva_full.astype(np.float32); yva = y_val

# ---- LightGBM (regularized + early stopping) ----
# We use the same model definition for a fair comparison
lgbm_model_pca = LGBMClassifier(
    objective="multiclass", class_weight=class_weights, num_leaves=31,
    min_child_samples=100, max_depth=-1, n_estimators=5000, learning_rate=0.06,
    feature_fraction=0.70, bagging_fraction=0.70, bagging_freq=1,
    lambda_l1=0.0, lambda_l2=10.0, min_split_gain=0.0, max_bin=63,
    n_jobs=-1, random_state=42, verbose=-1
)

lgbm_model_pca.fit(
    Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
)
status_pca = report("LightGBM (After PCA)", lgbm_model_pca, Xtr, ytr, Xva, yva, verbose=True)

# ---- Auto-tighten once if still overfitting ----
if status_pca == "Overfitting":
    print("\n[Auto-tighten on PCA] Reducing leaves, raising min_child_samples...")
    lgbm_model_pca.set_params(
        num_leaves=max(15, lgbm_model_pca.get_params()['num_leaves']//2),
        min_child_samples=min(300, lgbm_model_pca.get_params()['min_child_samples']*2),
        lambda_l2=max(20.0, lgbm_model_pca.get_params()['lambda_l2']*2),
    )
    lgbm_model_pca.fit(
        Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
    )
    report("LightGBM (PCA, auto-tightened)", lgbm_model_pca, Xtr, ytr, Xva, yva, verbose=True)

"""PCA did not help because of the following reasons:

It tells us that while PCA reduced the number of features, it likely threw away some of the subtle, non-linear information that the model was using to distinguish between emotions. <br>
The model is still overfitting because even with fewer features, it can't find a clear, generalizable pattern.

The Next Step in the Journey: Different Feature Engineering Approach

*   Since both flattening the spectrogram and using PCA still resulted in overfitting, the next logical step is to change the feature representation and create a more robust, hand-crafted feature set.
*   Instead of using the whole spectrogram "image," we will now calculate summary statistics for each of the 128 Mel frequency bands over time. This is called Feature Aggregation.


```
For each frequency band, we'll calculate its:

Mean (average energy)

Standard Deviation (how much the energy varies)

Maximum and Minimum values
```




This creates a much smaller, more stable feature set (~512 features) that is often more resilient to overfitting.
"""

# =================================================================
# JOURNEY STEP 4: Feature Aggregation
# =================================================================


print("--- Starting Feature Aggregation Journey ---")

def load_and_aggregate_features(df):
    """
    Loads spectrograms and aggregates them by calculating statistics
    across the time axis for each Mel band.
    """
    features = []
    for feature_path in tqdm(df['feature_path'], desc="Loading & Aggregating Features"):
        spectrogram = np.load(feature_path)

        # --- AGGREGATION STEP ---
        mean = np.mean(spectrogram, axis=1)
        std = np.std(spectrogram, axis=1)
        max_val = np.max(spectrogram, axis=1)
        min_val = np.min(spectrogram, axis=1)

        # Concatenate the stats to form a single feature vector
        # This results in 128*4 = 512 features
        aggregated_features = np.concatenate((mean, std, max_val, min_val))
        features.append(aggregated_features)

    return np.array(features)

# 1. Create the new aggregated feature sets
print("\n[1/3] Creating aggregated feature sets...")
X_train_agg = load_and_aggregate_features(train_df)
X_val_agg = load_and_aggregate_features(val_df)
X_test_agg = load_and_aggregate_features(test_df)

# 2. Scale the new aggregated features
print("\n[2/3] Scaling the new aggregated features...")
scaler_agg = StandardScaler()
X_train_agg_scaled = scaler_agg.fit_transform(X_train_agg)
X_val_agg_scaled = scaler_agg.transform(X_val_agg)
X_test_agg_scaled = scaler_agg.transform(X_test_agg)
print("Scaling complete.")
print(f"New aggregated feature shape: {X_train_agg_scaled.shape}")


# 3. Train the same model on these new features
print("\n[3/3] Training LightGBM on aggregated features...")
Xtr_full = X_train_agg_scaled
Xva_full = X_val_agg_scaled

IDX = stratified_cap_indices(Xtr_full, y_train, cap=8000)
Xtr, ytr = Xtr_full[IDX].astype(np.float32), y_train[IDX]
Xva       = Xva_full.astype(np.float32); yva = y_val

# We use the same robust model definition
lgbm_model_agg = LGBMClassifier(
    objective="multiclass", class_weight=class_weights, num_leaves=31,
    min_child_samples=100, max_depth=-1, n_estimators=5000, learning_rate=0.06,
    feature_fraction=0.70, bagging_fraction=0.70, bagging_freq=1,
    lambda_l1=0.0, lambda_l2=10.0, min_split_gain=0.0, max_bin=63,
    n_jobs=-1, random_state=42, verbose=-1
)

lgbm_model_agg.fit(
    Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
)
status_agg = report("LightGBM (After Aggregation)", lgbm_model_agg, Xtr, ytr, Xva, yva, verbose=True)

# ---- Auto-tighten if needed ----
if status_agg == "Overfitting":
    print("\n[Auto-tighten on Aggregated] Reducing leaves, raising min_child_samples...")
    lgbm_model_agg.set_params(
        num_leaves=max(15, lgbm_model_agg.get_params()['num_leaves']//2),
        min_child_samples=min(300, lgbm_model_agg.get_params()['min_child_samples']*2),
        lambda_l2=max(20.0, lgbm_model_agg.get_params()['lambda_l2']*2),
    )
    lgbm_model_agg.fit(
        Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
    )
    report("LightGBM (Aggregation, auto-tightened)", lgbm_model_agg, Xtr, ytr, Xva, yva, verbose=True)

"""Because of a small improvement, we will now implement a more advanced feature engineering step <br>
We will now enhance our load_and_aggregate_features function to extract not just Mel spectrogram stats, but also: <br>

*   MFCCs (Mel-Frequency Cepstral Coefficients): The gold standard in speech and voice recognition. They capture the unique timbral quality (the "texture" or "color") of a voice.
*   Chroma Features: These relate to the 12 musical pitch classes (C, C#, D, etc.). They are excellent for capturing the melodic and harmonic content of speech, which varies significantly with emotion.

Spectral Contrast: This measures the difference between the peaks and valleys in the sound's spectrum. It helps distinguish between clear, tonal sounds and noisy, breathy sounds.

By combining these with our existing Mel stats, we create a much more comprehensive "fingerprint" of each audio file for the model to learn from.
"""

def load_and_extract_advanced_features(df):
    """
    Loads spectrograms and extracts a rich set of aggregated features,
    including MFCCs, Chroma, and Spectral Contrast.
    """
    features = []
    for feature_path in tqdm(df['feature_path'], desc="Loading & Extracting Advanced Features"):
        # We don't need the saved spectrogram for this, we re-load the audio
        # to ensure we can calculate all feature types from the raw wave.
        # This assumes a 'path' column exists in the dataframe.
        y, sr = librosa.load(df.loc[df['feature_path'] == feature_path, 'path'].iloc[0], sr=TARGET_SR)

        # --- ADVANCED FEATURE EXTRACTION ---

        # 1. Mel Spectrogram (as before)
        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
        mel_db = librosa.power_to_db(mel_spectrogram, ref=np.max)

        # 2. MFCCs
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)

        # 3. Chroma Features
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)

        # 4. Spectral Contrast
        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

        # --- AGGREGATION ---
        # Aggregate each feature type
        mel_mean = np.mean(mel_db, axis=1)
        mel_std = np.std(mel_db, axis=1)
        mfccs_mean = np.mean(mfccs, axis=1)
        mfccs_std = np.std(mfccs, axis=1)
        chroma_mean = np.mean(chroma, axis=1)
        chroma_std = np.std(chroma, axis=1)
        contrast_mean = np.mean(contrast, axis=1)
        contrast_std = np.std(contrast, axis=1)

        # Concatenate all aggregated features into a single vector
        advanced_features = np.concatenate((
            mel_mean, mel_std,
            mfccs_mean, mfccs_std,
            chroma_mean, chroma_std,
            contrast_mean, contrast_std
        ))
        features.append(advanced_features)

    return np.array(features)

# 1. Create the new advanced feature sets
print("\n[1/3] Creating advanced feature sets...")
X_train_adv = load_and_extract_advanced_features(train_df)
X_val_adv = load_and_extract_advanced_features(val_df)
X_test_adv = load_and_extract_advanced_features(test_df)

# 2. Scale the new advanced features
print("\n[2/3] Scaling the new advanced features...")
scaler_adv = StandardScaler()
X_train_adv_scaled = scaler_adv.fit_transform(X_train_adv)
X_val_adv_scaled = scaler_adv.transform(X_val_adv)
X_test_adv_scaled = scaler_adv.transform(X_test_adv)
print("Scaling complete.")
print(f"New advanced feature shape: {X_train_adv_scaled.shape}")


# 3. Train the same model on these new, richer features
print("\n[3/3] Training LightGBM on advanced features...")
Xtr_full = X_train_adv_scaled
Xva_full = X_val_adv_scaled

IDX = stratified_cap_indices(Xtr_full, y_train, cap=8000)
Xtr, ytr = Xtr_full[IDX].astype(np.float32), y_train[IDX]
Xva       = Xva_full.astype(np.float32); yva = y_val

# We use the same robust model definition
lgbm_model_adv = LGBMClassifier(
    objective="multiclass", class_weight=class_weights, num_leaves=31,
    min_child_samples=100, max_depth=-1, n_estimators=5000, learning_rate=0.06,
    feature_fraction=0.70, bagging_fraction=0.70, bagging_freq=1,
    lambda_l1=0.0, lambda_l2=10.0, min_split_gain=0.0, max_bin=63,
    n_jobs=-1, random_state=42, verbose=-1
)

lgbm_model_adv.fit(
    Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
)
status_adv = report("LightGBM (Advanced Features)", lgbm_model_adv, Xtr, ytr, Xva, yva, verbose=True)

# ---- Auto-tighten if needed ----
if status_adv == "Overfitting":
    print("\n[Auto-tighten on Advanced] Reducing leaves, raising min_child_samples...")
    lgbm_model_adv.set_params(
        num_leaves=max(15, lgbm_model_adv.get_params()['num_leaves']//2),
        min_child_samples=min(300, lgbm_model_adv.get_params()['min_child_samples']*2),
        lambda_l2=max(20.0, lgbm_model_adv.get_params()['lambda_l2']*2),
    )
    lgbm_model_adv.fit(
        Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="multi_logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
    )
    report("LightGBM (Advanced, auto-tightened)", lgbm_model_adv, Xtr, ytr, Xva, yva, verbose=True)

"""Key Improvements:
1. Feature Engineering Revolution

*   Smaller, More Meaningful Features: Instead of 7951 features, we extract ~118 highly informative features
*   Multi-domain Features: Combines temporal, frequency, MFCC, delta-MFCC, and pitch features

*   Reduced Dimensionality: Prevents the model from memorizing noise

2. Extreme Regularization

*   LightGBM: Reduced to only 8 leaves, depth of 3, very high L2 regularization (50.0)
*   High minimum samples: Requires 200 samples per leaf

*   Lower learning rate: 0.01 instead of 0.06

3. Model Diversity

*   Linear SVM: Naturally resistant to overfitting with linear boundaries
*   RBF SVM: Non-linear but with strong regularization (C=0.5)
*   KNN: Simple non-parametric approach
*   Random Forest: Extremely shallow trees (depth=5)

4. Ensemble Methods

*   Voting Classifier: Combines predictions from multiple models
*   Bagging: Creates multiple versions of the best model for stability




"""



# =================================================================
# JOURNEY STEP 6: ROBUST MULTI-STRATEGY IMPROVEMENT
# =================================================================

print("\n" + "="*80)
print("JOURNEY STEP 6: ROBUST MULTI-STRATEGY APPROACH")
print("="*80)
print("Previous best: Train 92.58% / Val 50.84% (Severe Overfitting)")
print("Goal: Drastically reduce overfitting and improve validation accuracy")
print("="*80)

# --- Strategy 1: Ensemble of Different Feature Types ---
print("\n[Strategy 1] Creating Ensemble Feature Set...")

def extract_temporal_features(y, sr):
    """Extract temporal domain features"""
    # Zero crossing rate
    zcr = librosa.feature.zero_crossing_rate(y)
    # Energy
    energy = np.sum(y**2) / len(y)
    # RMS
    rms = librosa.feature.rms(y=y)

    return np.concatenate([
        [np.mean(zcr), np.std(zcr)],
        [energy],
        [np.mean(rms), np.std(rms)]
    ])

def extract_frequency_features(y, sr):
    """Extract frequency domain features"""
    # Spectral centroid
    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)
    # Spectral rolloff
    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
    # Spectral bandwidth
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)

    return np.concatenate([
        [np.mean(spectral_centroids), np.std(spectral_centroids)],
        [np.mean(spectral_rolloff), np.std(spectral_rolloff)],
        [np.mean(spectral_bandwidth), np.std(spectral_bandwidth)]
    ])

def extract_robust_features(df, use_cache=True):
    """
    Extract a smaller, more robust feature set combining multiple approaches
    """
    cache_file = f"robust_features_{len(df)}.npy"

    if use_cache and os.path.exists(cache_file):
        print(f"Loading cached features from {cache_file}")
        return np.load(cache_file)

    features = []
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Extracting Robust Features"):
        try:
            # Load the original audio
            y, sr = librosa.load(row['path'], sr=TARGET_SR)

            # 1. Core MFCC features (reduced from 40 to 13)
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            mfcc_features = np.concatenate([
                np.mean(mfccs, axis=1),
                np.std(mfccs, axis=1),
                np.max(mfccs, axis=1),
                np.min(mfccs, axis=1)
            ])

            # 2. Temporal features
            temporal_features = extract_temporal_features(y, sr)

            # 3. Frequency features
            frequency_features = extract_frequency_features(y, sr)

            # 4. Delta and Delta-Delta MFCCs (captures temporal dynamics)
            mfcc_delta = librosa.feature.delta(mfccs)
            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)

            delta_features = np.concatenate([
                np.mean(mfcc_delta, axis=1),
                np.std(mfcc_delta, axis=1),
                np.mean(mfcc_delta2, axis=1),
                np.std(mfcc_delta2, axis=1)
            ])

            # 5. Pitch features (fundamental frequency)
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
            pitch_features = np.array([
                np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0,
                np.std(pitches[pitches > 0]) if np.any(pitches > 0) else 0
            ])

            # Combine all features
            combined_features = np.concatenate([
                mfcc_features,
                temporal_features,
                frequency_features,
                delta_features,
                pitch_features
            ])

            features.append(combined_features)

        except Exception as e:
            print(f"Error processing {row['path']}: {e}")
            # Return zeros if error
            features.append(np.zeros(118))  # Adjust size based on feature count

    features = np.array(features)

    if use_cache:
        np.save(cache_file, features)
        print(f"Cached features to {cache_file}")

    return features

# Extract robust features
print("\nExtracting robust features for all datasets...")
X_train_robust = extract_robust_features(train_df, use_cache=True)
X_val_robust = extract_robust_features(val_df, use_cache=True)
X_test_robust = extract_robust_features(test_df, use_cache=True)

print(f"Robust feature shape: {X_train_robust.shape}")

# Scale the robust features
print("\nScaling robust features...")
scaler_robust = StandardScaler()
X_train_robust_scaled = scaler_robust.fit_transform(X_train_robust)
X_val_robust_scaled = scaler_robust.transform(X_val_robust)
X_test_robust_scaled = scaler_robust.transform(X_test_robust)

# --- Strategy 2: Extreme Regularization with Ensemble ---
print("\n[Strategy 2] Training Ensemble with Extreme Regularization...")

from sklearn.ensemble import VotingClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Prepare data
Xtr = X_train_robust_scaled
Xva = X_val_robust_scaled
ytr = y_train
yva = y_val

# Model 1: Extremely regularized LightGBM
lgbm_extreme = LGBMClassifier(
    objective="multiclass",
    class_weight=class_weights,
    num_leaves=8,  # Very small
    min_child_samples=200,  # Very high
    max_depth=3,  # Very shallow
    n_estimators=1000,
    learning_rate=0.01,  # Very low
    feature_fraction=0.5,
    bagging_fraction=0.5,
    bagging_freq=1,
    lambda_l1=5.0,  # High L1
    lambda_l2=50.0,  # Very high L2
    min_split_gain=0.5,  # High threshold
    max_bin=31,  # Small bins
    n_jobs=-1,
    random_state=42,
    verbose=-1
)

# Model 2: Linear SVM (naturally resistant to overfitting)
svm_linear = SVC(
    kernel='linear',
    C=0.1,  # Low C = high regularization
    class_weight=class_weights,
    probability=True,
    random_state=42
)

# Model 3: RBF SVM with strong regularization
svm_rbf = SVC(
    kernel='rbf',
    C=0.5,
    gamma='scale',
    class_weight=class_weights,
    probability=True,
    random_state=42
)

# Model 4: Simple KNN (non-parametric, simple)
knn = KNeighborsClassifier(
    n_neighbors=15,
    weights='distance',
    metric='minkowski',
    p=2
)

# Model 5: Extremely pruned Random Forest
rf_extreme = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,  # Very shallow
    min_samples_leaf=20,  # High minimum
    min_samples_split=50,  # High minimum
    max_features='sqrt',
    class_weight=class_weights,
    max_samples=0.5,  # Only use 50% of data per tree
    n_jobs=-1,
    random_state=42
)

print("\nTraining individual models...")

# Train LightGBM with early stopping
lgbm_extreme.fit(
    Xtr, ytr,
    eval_set=[(Xva, yva)],
    eval_metric="multi_logloss",
    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
)

# Train other models
print("Training SVM Linear...")
svm_linear.fit(Xtr, ytr)

print("Training SVM RBF...")
svm_rbf.fit(Xtr, ytr)

print("Training KNN...")
knn.fit(Xtr, ytr)

print("Training Random Forest...")
rf_extreme.fit(Xtr, ytr)

# Evaluate individual models
models_dict = {
    "LightGBM Extreme": lgbm_extreme,
    "SVM Linear": svm_linear,
    "SVM RBF": svm_rbf,
    "KNN": knn,
    "RF Extreme": rf_extreme
}

print("\n" + "="*80)
print("INDIVIDUAL MODEL PERFORMANCE")
print("="*80)

best_val_acc = 0
best_model_name = ""
best_model = None

for name, model in models_dict.items():
    y_tr_pred = model.predict(Xtr)
    y_va_pred = model.predict(Xva)

    acc_tr = accuracy_score(ytr, y_tr_pred)
    acc_va = accuracy_score(yva, y_va_pred)
    f1_tr = f1_score(ytr, y_tr_pred, average='macro')
    f1_va = f1_score(yva, y_va_pred, average='macro')

    gap = acc_tr - acc_va

    print(f"\n{name}:")
    print(f"  Train: Acc={acc_tr*100:.2f}%, F1={f1_tr*100:.2f}%")
    print(f"  Val:   Acc={acc_va*100:.2f}%, F1={f1_va*100:.2f}%")
    print(f"  Gap:   {gap*100:.2f} pp")

    if acc_va > best_val_acc:
        best_val_acc = acc_va
        best_model_name = name
        best_model = model

print(f"\nBest individual model: {best_model_name} with {best_val_acc*100:.2f}% validation accuracy")

# --- Strategy 3: Weighted Ensemble ---
print("\n[Strategy 3] Creating Weighted Ensemble...")

# Create ensemble with best performers
ensemble = VotingClassifier(
    estimators=[
        ('lgbm', lgbm_extreme),
        ('svm_linear', svm_linear),
        ('rf', rf_extreme)
    ],
    voting='soft',  # Use probabilities
    weights=[2, 1, 1]  # Give more weight to LightGBM if it performs better
)

print("Training ensemble...")
ensemble.fit(Xtr, ytr)

# Evaluate ensemble
y_tr_ens = ensemble.predict(Xtr)
y_va_ens = ensemble.predict(Xva)

acc_tr_ens = accuracy_score(ytr, y_tr_ens)
acc_va_ens = accuracy_score(yva, y_va_ens)
f1_tr_ens = f1_score(ytr, y_tr_ens, average='macro')
f1_va_ens = f1_score(yva, y_va_ens, average='macro')

print("\n" + "="*80)
print("ENSEMBLE PERFORMANCE")
print("="*80)
print(f"Train: Acc={acc_tr_ens*100:.2f}%, F1={f1_tr_ens*100:.2f}%")
print(f"Val:   Acc={acc_va_ens*100:.2f}%, F1={f1_va_ens*100:.2f}%")
print(f"Gap:   {(acc_tr_ens - acc_va_ens)*100:.2f} pp")

# --- Strategy 4: Bagging for Stability ---
print("\n[Strategy 4] Bagging the Best Model for Stability...")

bagging_model = BaggingClassifier(
    estimator=lgbm_extreme,
    n_estimators=10,
    max_samples=0.8,
    max_features=0.8,
    bootstrap=True,
    bootstrap_features=False,
    n_jobs=-1,
    random_state=42
)

print("Training bagged model...")
bagging_model.fit(Xtr, ytr)

y_tr_bag = bagging_model.predict(Xtr)
y_va_bag = bagging_model.predict(Xva)

acc_tr_bag = accuracy_score(ytr, y_tr_bag)
acc_va_bag = accuracy_score(yva, y_va_bag)
f1_tr_bag = f1_score(ytr, y_tr_bag, average='macro')
f1_va_bag = f1_score(yva, y_va_bag, average='macro')

print("\n" + "="*80)
print("BAGGING PERFORMANCE")
print("="*80)
print(f"Train: Acc={acc_tr_bag*100:.2f}%, F1={f1_tr_bag*100:.2f}%")
print(f"Val:   Acc={acc_va_bag*100:.2f}%, F1={f1_va_bag*100:.2f}%")
print(f"Gap:   {(acc_tr_bag - acc_va_bag)*100:.2f} pp")

# --- Final Model Selection ---
print("\n" + "="*80)
print("FINAL MODEL SELECTION")
print("="*80)

final_models = {
    "Ensemble": (acc_va_ens, ensemble),
    "Bagging": (acc_va_bag, bagging_model),
    best_model_name: (best_val_acc, best_model)
}

best_final_acc = 0
best_final_name = ""
best_final_model = None

for name, (acc, model) in final_models.items():
    if acc > best_final_acc:
        best_final_acc = acc
        best_final_name = name
        best_final_model = model

print(f"\nBest overall model: {best_final_name} with {best_final_acc*100:.2f}% validation accuracy")

# Detailed evaluation of best model
print(f"\n{best_final_name} - Detailed Report:")
y_va_final = best_final_model.predict(Xva)
print(classification_report(yva, y_va_final, target_names=label_encoder.classes_))

# Confusion matrix
cm = confusion_matrix(yva, y_va_final)
cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)
print("\nConfusion Matrix:")
display(cm_df)

print("\n" + "="*80)
print("IMPROVEMENT SUMMARY")
print("="*80)
print(f"Previous: Train 92.58% / Val 50.84% (Gap: 41.74 pp)")
print(f"Current:  Best validation {best_final_acc*100:.2f}%")
print(f"Improvement: {(best_final_acc - 0.5084)*100:.2f} pp in validation accuracy")
print("="*80)

"""Saving the model"""

# =================================================================
# DEPLOYMENT PREPARATION: Save Model as finalMLnn to Google Drive (FIXED)
# =================================================================

import pickle
import joblib  # Alternative to pickle, often better for scikit-learn objects
from datetime import datetime
import json
import shutil

print("\n" + "="*80)
print("DEPLOYMENT PREPARATION: Saving Models and Components as finalMLnn")
print("="*80)

# --- Create deployment directory in Google Drive ---
# Using the Google Drive path from your screenshot
DRIVE_DEPLOYMENT_DIR = "/content/drive/MyDrive/multiclassproject/pickle files"
os.makedirs(DRIVE_DEPLOYMENT_DIR, exist_ok=True)

# Also create a local deployment directory for faster access
LOCAL_DEPLOYMENT_DIR = os.path.join(LOCAL_ROOT, "deployment")
os.makedirs(LOCAL_DEPLOYMENT_DIR, exist_ok=True)

print(f"Deployment files will be saved to:")
print(f"  Drive: {DRIVE_DEPLOYMENT_DIR}")
print(f"  Local: {LOCAL_DEPLOYMENT_DIR}")

# --- Determine which feature set and scaler were used with lgbm_model_adv ---
# The model expects 374 features, which matches the advanced features
# Let's verify and use the correct feature set

print("\n[Checking model dimensions...]")
print(f"Model expects: {lgbm_model_adv.n_features_in_} features")
print(f"X_train_adv shape: {X_train_adv_scaled.shape}")
print(f"Xtr_full shape: {Xtr_full.shape}")
print(f"Xva_full shape: {Xva_full.shape}")

# Use the full advanced feature sets that match the model
if lgbm_model_adv.n_features_in_ == X_train_adv_scaled.shape[1]:
    print("✓ Using advanced feature set (X_train_adv_scaled)")
    X_train_final = X_train_adv_scaled
    X_val_final = X_val_adv_scaled
    scaler_final = scaler_adv
    feature_type = "advanced"
else:
    # Fallback: try to determine the correct feature set
    print("⚠ Feature dimension mismatch. Using available feature set.")
    X_train_final = Xtr_full
    X_val_final = Xva_full
    scaler_final = scaler_adv
    feature_type = "custom"

# --- Prepare model metadata ---
# Calculate accuracies using the correct feature sets
try:
    # Try with full dataset first
    train_acc = float(accuracy_score(y_train, lgbm_model_adv.predict(X_train_final)))
    val_acc = float(accuracy_score(y_val, lgbm_model_adv.predict(X_val_final)))
except:
    # If that fails, use the subset that was actually used for training
    train_acc = float(accuracy_score(ytr, lgbm_model_adv.predict(Xtr_full)))
    val_acc = float(accuracy_score(yva, lgbm_model_adv.predict(Xva_full)))

model_metadata = {
    "model_name": "finalMLnn",
    "project_name": PROJECT_NAME,
    "dataset_name": DATASET_NAME,
    "creation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "model_type": "LightGBM with Advanced Features",
    "training_accuracy": train_acc,
    "validation_accuracy": val_acc,
    "n_classes": len(label_encoder.classes_),
    "emotion_classes": label_encoder.classes_.tolist(),
    "feature_dimensions": lgbm_model_adv.n_features_in_,
    "feature_type": feature_type,
    "feature_extraction": {
        "target_sr": TARGET_SR,
        "n_mfcc": 40,
        "n_mels": 128,
        "n_chroma": 12,
        "n_contrast": 7,
        "aggregations": ["mean", "std"],
        "total_features": lgbm_model_adv.n_features_in_
    },
    "preprocessing": {
        "target_amplitude": TARGET_AMP,
        "trim_db": TRIM_DB,
        "n_fft": N_FFT if 'N_FFT' in globals() else 2048,
        "hop_length": HOP_LENGTH if 'HOP_LENGTH' in globals() else 512
    },
    "training_samples": len(y_train),
    "validation_samples": len(y_val),
    "test_samples": len(y_test),
    "datasets_used": ["RAVDESS", "TESS", "CREMA-D"]
}

print(f"\n✓ Model metadata prepared")
print(f"  Training Accuracy: {train_acc*100:.2f}%")
print(f"  Validation Accuracy: {val_acc*100:.2f}%")

# --- Save to LOCAL first (faster), then copy to Drive ---

# 1. Save the main model as finalMLnn.pkl
print("\n[1/7] Saving trained model as finalMLnn...")
model_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn.pkl")
with open(model_path_local, 'wb') as f:
    pickle.dump(lgbm_model_adv, f, protocol=pickle.HIGHEST_PROTOCOL)
print(f"✓ Model saved locally: {model_path_local} ({os.path.getsize(model_path_local)/1024:.2f} KB)")

# Also save with joblib format
model_joblib_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn.joblib")
joblib.dump(lgbm_model_adv, model_joblib_path_local)
print(f"✓ Model (joblib) saved locally: {model_joblib_path_local} ({os.path.getsize(model_joblib_path_local)/1024:.2f} KB)")

# 2. Save feature scaler
print("\n[2/7] Saving feature scaler...")
scaler_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn_scaler.pkl")
with open(scaler_path_local, 'wb') as f:
    pickle.dump(scaler_final, f, protocol=pickle.HIGHEST_PROTOCOL)
print(f"✓ Scaler saved: {scaler_path_local} ({os.path.getsize(scaler_path_local)/1024:.2f} KB)")

# 3. Save label encoder
print("\n[3/7] Saving label encoder...")
label_encoder_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn_label_encoder.pkl")
with open(label_encoder_path_local, 'wb') as f:
    pickle.dump(label_encoder, f, protocol=pickle.HIGHEST_PROTOCOL)
print(f"✓ Label encoder saved: {label_encoder_path_local}")

# 4. Save model metadata
print("\n[4/7] Saving model metadata...")
metadata_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn_metadata.json")
with open(metadata_path_local, 'w') as f:
    json.dump(model_metadata, f, indent=4)
print(f"✓ Metadata saved: {metadata_path_local}")

# 5. Save class weights
print("\n[5/7] Saving class weights...")
class_weights_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn_class_weights.pkl")
with open(class_weights_path_local, 'wb') as f:
    pickle.dump(class_weights, f)
print(f"✓ Class weights saved: {class_weights_path_local}")

# 6. Create a comprehensive deployment bundle named finalMLnn_bundle
print("\n[6/7] Creating complete deployment bundle...")

# Store the feature extraction function if it exists
feature_extraction_func = None
if 'load_and_extract_advanced_features' in globals():
    # We can't pickle the function directly, but we can save its code
    import inspect
    feature_extraction_code = inspect.getsource(load_and_extract_advanced_features)
else:
    feature_extraction_code = None

deployment_bundle = {
    "model_name": "finalMLnn",
    "model": lgbm_model_adv,
    "scaler": scaler_final,
    "label_encoder": label_encoder,
    "class_weights": class_weights,
    "metadata": model_metadata,
    "feature_extraction_params": {
        "target_sr": TARGET_SR,
        "n_mfcc": 40,
        "n_mels": 128,
        "n_chroma": 12,
        "n_contrast": 7,
        "n_features": lgbm_model_adv.n_features_in_
    },
    "preprocessing_params": {
        "target_amplitude": TARGET_AMP,
        "trim_db": TRIM_DB
    },
    "feature_extraction_code": feature_extraction_code
}

bundle_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn_bundle.pkl")
with open(bundle_path_local, 'wb') as f:
    pickle.dump(deployment_bundle, f, protocol=pickle.HIGHEST_PROTOCOL)
print(f"✓ Complete bundle saved: {bundle_path_local} ({os.path.getsize(bundle_path_local)/1024:.2f} KB)")

# --- Copy all files to Google Drive ---
print("\n[7/7] Copying deployment files to Google Drive...")

files_to_copy = [
    ("finalMLnn.pkl", model_path_local),
    ("finalMLnn.joblib", model_joblib_path_local),
    ("finalMLnn_scaler.pkl", scaler_path_local),
    ("finalMLnn_label_encoder.pkl", label_encoder_path_local),
    ("finalMLnn_metadata.json", metadata_path_local),
    ("finalMLnn_class_weights.pkl", class_weights_path_local),
    ("finalMLnn_bundle.pkl", bundle_path_local)
]

for filename, source_path in files_to_copy:
    dest_path = os.path.join(DRIVE_DEPLOYMENT_DIR, filename)
    shutil.copy2(source_path, dest_path)
    print(f"✓ Copied {filename} to Drive")

# --- Create prediction script specifically for finalMLnn ---
print("\n" + "="*80)
print("Creating Deployment Prediction Function for finalMLnn")
print("="*80)

prediction_code = '''#!/usr/bin/env python3
"""
Speech Emotion Recognition - finalMLnn Prediction Module
Generated: {timestamp}
Model: finalMLnn
Expected Features: {n_features}
"""

import pickle
import numpy as np
import librosa
import warnings
warnings.filterwarnings('ignore')

class FinalMLnnPredictor:
    """Production-ready emotion prediction using finalMLnn model."""

    def __init__(self, bundle_path="finalMLnn_bundle.pkl"):
        """Load the finalMLnn deployment bundle."""
        with open(bundle_path, 'rb') as f:
            self.bundle = pickle.load(f)

        self.model_name = self.bundle.get('model_name', 'finalMLnn')
        self.model = self.bundle['model']
        self.scaler = self.bundle['scaler']
        self.label_encoder = self.bundle['label_encoder']
        self.params = self.bundle['feature_extraction_params']
        self.prep_params = self.bundle['preprocessing_params']
        self.expected_features = self.params['n_features']

        print(f"Loaded {{self.model_name}} model successfully!")
        print(f"Model expects {{self.expected_features}} features")

    def extract_features(self, audio_path):
        """Extract features from audio file matching training pipeline."""
        # Load audio
        y, sr = librosa.load(audio_path, sr=self.params['target_sr'])

        # Normalize
        max_amp = np.max(np.abs(y))
        if max_amp > 0:
            y = (y / max_amp) * self.prep_params['target_amplitude']

        # Trim silence
        y, _ = librosa.effects.trim(y, top_db=self.prep_params['trim_db'])

        # Extract features (matching the training pipeline exactly)
        # 1. Mel Spectrogram
        mel_spec = librosa.feature.melspectrogram(
            y=y, sr=sr, n_mels=self.params['n_mels']
        )
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)

        # 2. MFCCs
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.params['n_mfcc'])

        # 3. Chroma
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)

        # 4. Spectral Contrast
        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

        # Aggregate features
        features = np.concatenate([
            np.mean(mel_db, axis=1), np.std(mel_db, axis=1),
            np.mean(mfccs, axis=1), np.std(mfccs, axis=1),
            np.mean(chroma, axis=1), np.std(chroma, axis=1),
            np.mean(contrast, axis=1), np.std(contrast, axis=1)
        ])

        # Ensure we have the right number of features
        if len(features) != self.expected_features:
            print(f"Warning: Feature mismatch. Got {{len(features)}}, expected {{self.expected_features}}")
            # Pad or truncate as needed
            if len(features) < self.expected_features:
                features = np.pad(features, (0, self.expected_features - len(features)), 'constant')
            else:
                features = features[:self.expected_features]

        return features

    def predict(self, audio_path):
        """Predict emotion from audio file using finalMLnn model."""
        # Extract features
        features = self.extract_features(audio_path)

        # Scale features
        features_scaled = self.scaler.transform(features.reshape(1, -1))

        # Predict
        prediction = self.model.predict(features_scaled)[0]
        probabilities = self.model.predict_proba(features_scaled)[0]

        # Decode label
        emotion = self.label_encoder.inverse_transform([prediction])[0]

        # Create confidence dictionary
        confidence_dict = {{
            self.label_encoder.inverse_transform([i])[0]: float(prob)
            for i, prob in enumerate(probabilities)
        }}

        return {{
            'model': self.model_name,
            'predicted_emotion': emotion,
            'confidence': float(probabilities[prediction]),
            'all_probabilities': confidence_dict
        }}

    def predict_batch(self, audio_paths):
        """Predict emotions for multiple audio files."""
        results = []
        for path in audio_paths:
            try:
                result = self.predict(path)
                result['audio_path'] = path
                result['status'] = 'success'
            except Exception as e:
                result = {{
                    'audio_path': path,
                    'status': 'error',
                    'error_message': str(e)
                }}
            results.append(result)
        return results

# Example usage
if __name__ == "__main__":
    # Initialize predictor with finalMLnn model
    predictor = FinalMLnnPredictor('finalMLnn_bundle.pkl')

    # Example prediction
    audio_file = "path/to/your/audio.wav"
    result = predictor.predict(audio_file)

    print(f"Model: {{result['model']}}")
    print(f"Predicted emotion: {{result['predicted_emotion']}}")
    print(f"Confidence: {{result['confidence']:.2%%}}")
    print("\\nAll probabilities:")
    for emotion, prob in result['all_probabilities'].items():
        print(f"  {{emotion}}: {{prob:.2%%}}")
'''.format(
    timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    n_features=lgbm_model_adv.n_features_in_
)

# Save prediction script locally
prediction_script_path_local = os.path.join(LOCAL_DEPLOYMENT_DIR, "finalMLnn_predictor.py")
with open(prediction_script_path_local, 'w') as f:
    f.write(prediction_code)

# Copy to Drive
prediction_script_path_drive = os.path.join(DRIVE_DEPLOYMENT_DIR, "finalMLnn_predictor.py")
shutil.copy2(prediction_script_path_local, prediction_script_path_drive)
print(f"✓ Prediction script saved: finalMLnn_predictor.py")

# --- Test the saved model ---
print("\n" + "="*80)
print("Testing finalMLnn Model")
print("="*80)

# Load the saved model to verify
print("\nLoading finalMLnn for verification...")
with open(model_path_local, 'rb') as f:
    loaded_model = pickle.load(f)

with open(scaler_path_local, 'rb') as f:
    loaded_scaler = pickle.load(f)

with open(label_encoder_path_local, 'rb') as f:
    loaded_label_encoder = pickle.load(f)

# Test on a small validation subset using the correct feature set
test_size = min(100, len(X_val_final))
X_test_subset = X_val_final[:test_size]
y_test_subset = y_val[:test_size]

# Predictions with loaded model
y_pred_loaded = loaded_model.predict(X_test_subset)
accuracy_loaded = accuracy_score(y_test_subset, y_pred_loaded)

print(f"Original model validation accuracy: {val_acc*100:.2f}%")
print(f"Loaded finalMLnn test subset accuracy: {accuracy_loaded*100:.2f}%")
print("✓ finalMLnn model successfully saved and verified!")

# --- Summary ---
print("\n" + "="*80)
print("FINALMLNN DEPLOYMENT PREPARATION COMPLETE")
print("="*80)
print(f"\nAll finalMLnn files saved to:")
print(f"  Google Drive: {DRIVE_DEPLOYMENT_DIR}")
print(f"  Local backup: {LOCAL_DEPLOYMENT_DIR}")

print("\nFiles created:")
print("  1. finalMLnn.pkl              - Main model (pickle)")
print("  2. finalMLnn.joblib           - Main model (joblib)")
print("  3. finalMLnn_scaler.pkl       - Feature scaler")
print("  4. finalMLnn_label_encoder.pkl - Label encoder")
print("  5. finalMLnn_metadata.json    - Model metadata")
print("  6. finalMLnn_class_weights.pkl - Class weights")
print("  7. finalMLnn_bundle.pkl       - Complete bundle")
print("  8. finalMLnn_predictor.py     - Prediction script")

print("\n📊 finalMLnn Model Performance:")
print(f"  Training Accuracy:   {train_acc*100:.2f}%")
print(f"  Validation Accuracy: {val_acc*100:.2f}%")
print(f"  Number of Classes:   {model_metadata['n_classes']}")
print(f"  Feature Dimensions:  {model_metadata['feature_dimensions']}")

print("\n🚀 finalMLnn is ready for deployment!")
print("\nTo use the model:")
print("  from finalMLnn_predictor import FinalMLnnPredictor")
print("  predictor = FinalMLnnPredictor('finalMLnn_bundle.pkl')")
print("  result = predictor.predict('audio.wav')")
print("="*80)